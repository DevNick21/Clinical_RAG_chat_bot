\chapter{Results \& Discussion}
\label{chap:results}

\noindent This chapter reports the outcomes of our Clinical RAG evaluation and discusses the key findings. We reiterate the study aims: to quantify how different embedding models and small LLMs affect end-to-end retrieval-augmented answering quality, speed, and safety on a clinical QA set.

\section{Experiment Overview}
\begin{itemize}
  \item Total experiments: 54 runs (9 embedding models \(\times\) 6 LLMs).
  \item Embeddings: ms-marco, multi-qa, mini-lm, biomedbert, mpnet-v2, e5-base, BioLORD, BioBERT, MedQuAD.
  \item LLMs: deepseek, qwen, llama, gemma, phi3, tinyllama.
  \item Per-run questions: 20; metrics include pass rate, average score (primary), search time, documents found; efficiency/safety include throughput (QPM), disclaimer and hallucination rates.
  \item \textbf{Total Question-Answer Pairs}: 1,080 clinical evaluations
  \item \textbf{Evaluation Duration}: 30+ hours of automated testing
\end{itemize}

\section{Overall Performance}
Aggregate results across all 54 runs show:
\begin{itemize}
  \item Average pass rate: 89.6\%.
  \item Average score: 0.706.
  \item Average search time: 98.77s.
\end{itemize}

Best-performing configurations:
\begin{itemize}
  \item Highest overall score: BioBERT + phi3 (avg. score 0.770, 100\% pass rate).
  \item Fastest: e5-base + deepseek (avg. search time 53.28s, avg. score 0.640).
\end{itemize}

\subsection{Statistical Performance Overview}
\input{chap4_results/tables/system_statistics}

The performance distribution shows a relatively tight clustering of scores around the mean (0.706), with a standard deviation indicating consistent quality across configurations. Key statistical insights include:

\begin{itemize}
    \item \textbf{Score Distribution}: Near-normal distribution with slight positive skew
    \item \textbf{Pass Rate Consistency}: 89.6\% average with relatively low variance ($\sigma$=0.062)
    \item \textbf{Search Time Variability}: High variance ($\sigma$=98.7s) indicating significant performance differences
    \item \textbf{Coefficient of Variation}: Score consistency (6.2\%) vs. efficiency variability (99.8\%)
\end{itemize}

\section{Comparison Heatmap}
Figure~\ref{fig:heatmap_avg_score} provides a model-by-model comparison heatmap (average score).

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=\textwidth]{chap4_results/images/heatmap_average_score.png}
  \caption{Model comparison heatmap by average score.}
  \label{fig:heatmap_avg_score}
\end{figure}

\section{Top-Performing Configurations}

\input{chap4_results/tables/enhanced_top_performers}

\subsection{Multi-Dimensional Performance Assessment}

The top-performing configurations demonstrate different optimization strategies:

\textbf{Quality-Optimized (BioBERT + Phi3):}
\begin{itemize}
    \item Highest overall score (0.770)
    \item Perfect reliability (100\% pass rate)
    \item Moderate speed (67.7s)
    \item Medical domain specialization advantage
    \item Low hallucination rate (15\%)
\end{itemize}

\textbf{Balance-Optimized (multi-qa + llama):}
\begin{itemize}
    \item High score (0.764) with excellent speed (61.2s)
    \item Strong reliability (95\% pass rate)
    \item General-purpose efficiency
    \item Optimal speed-quality balance
\end{itemize}

\textbf{Reliability-Optimized (mini-lm + llama):}
\begin{itemize}
    \item Perfect reliability (100\% pass rate)
    \item Good score (0.751)
    \item Fast retrieval (56.5s)
    \item Consistent performance across categories
\end{itemize}

\paragraph{Observations.} The best average scores are achieved by BioBERT + phi3 and multi-qa + phi3/llama. MiniLM pairs (mini-lm + llama/deepseek) are strong with excellent pass rates; however, mini-lm + deepseek exhibits much longer search time, suggesting backend or retrieval interaction overhead. MedQuAD-based embeddings produce competitive average scores, but at times with slower end-to-end latency.

\section{Component-Wise Performance Analysis}

\subsection{Embedding Model Performance}
\input{chap4_results/tables/embedding_ranking}

\textbf{Medical Specialization vs. Generalization Trade-off:}
\begin{itemize}
    \item \textbf{Multi-qa-mpnet} (0.730): Top performer - QA specialization trumps domain knowledge
    \item \textbf{MPNet-v2} (0.718): Strong general-purpose performance
    \item \textbf{Mini-lm} (0.716): Efficient lightweight option
    \item \textbf{BioBERT} (0.716): Medical specialization shows competitive results
    \item \textbf{Medical Models} generally: Excel in accuracy but slower retrieval
    \item \textbf{General Models}: Faster retrieval, competitive accuracy
\end{itemize}

\textbf{Key Finding}: Question-answering specialization (multi-qa embeddings) proves more valuable than medical domain specialization alone, suggesting that RAG systems benefit more from retrieval task alignment than domain knowledge pre-training.

\subsection{Large Language Model Performance Analysis}

LLM performance shows less variation than embedding models, with scores ranging from 0.696 (qwen) to 0.717 (tinyllama). This narrow range suggests that the embedding component has greater impact on overall system performance than LLM selection.

\textbf{Model Size vs Performance Correlation:}
\begin{itemize}
    \item \textbf{TinyLlama (1.1B)}: 0.717 - Highest average performance despite smallest size
    \item \textbf{Llama3.2}: 0.716 - Strong performance from instruction tuning
    \item \textbf{Phi3 (3.8B)}: 0.709 - Largest model, moderate performance
    \item \textbf{DeepSeek-R1 (1.5B)}: 0.701 - Reasoning focus, competitive results
\end{itemize}

\textbf{Surprising Result}: The smallest model (TinyLlama) achieved the highest average performance, challenging assumptions about parameter count correlating with quality in specialized domains.

\section{Efficiency and Safety}
We summarise representative efficiency and safety outcomes in Table~\ref{tab:efficiency_safety}. Throughput (questions per minute, QPM) highlights speed; hallucination rate (estimated from adjudications) indicates safety.

\begin{table}[!htbp]
\centering
\caption{Efficiency and safety metrics.}
\label{tab:efficiency_safety}
\begin{footnotesize}
\renewcommand\arraystretch{0.95}
\begin{tabularx}{0.9\textwidth}{l l X X X}
  \toprule
  Embedding & LLM & QPM & Avg. score & Hallucination \\
  \midrule
  e5-base & deepseek & 1.122 & 0.640 & 0.40 \\
  mini-lm & qwen & 1.100 & 0.703 & 0.30 \\
  mini-lm & llama & 1.059 & 0.751 & 0.15 \\
  MedQuAD & deepseek & 0.169 & 0.744 & 0.10 \\
  \bottomrule
\end{tabularx}
\end{footnotesize}
\end{table}

\subsection{Safety and Hallucination Analysis}

All configurations maintain hallucination rates below 45\%, with medical-specialized combinations showing superior safety profiles:

\begin{itemize}
    \item \textbf{Safest Configuration}: MedQuAD + deepseek (10\% hallucination, 0.744 score)
    \item \textbf{Balanced Safety}: BioBERT + phi3 (15\% hallucination, 0.770 score)
    \item \textbf{Speed vs Safety Trade-off}: Faster configurations tend toward higher hallucination rates
    \item \textbf{Disclaimer Generation}: 100\% across all models, demonstrating appropriate clinical caution
\end{itemize}

\paragraph{Trade-offs.} The fastest pipeline (e5-base + deepseek) sacrifices some answer quality relative to the top-scoring setups. Mini-lm + llama offers an appealing balance: perfect pass rate, good average score, high throughput, and low hallucination. The safest configuration by hallucination rate (MedQuAD + deepseek) is slower; this may reflect conservative generation behaviour or heavier retrieval.

\section{Clinical Deployment Recommendations}

Based on comprehensive performance analysis, we provide evidence-based recommendations for different clinical deployment scenarios:

\subsection{High-Accuracy Clinical Decision Support}
\textbf{Recommended Configuration}: BioBERT + Phi3
\begin{itemize}
    \item \textbf{Use Case}: Complex diagnostic assistance, treatment planning
    \item \textbf{Performance}: 0.770 score, 100\% pass rate, 15\% hallucination
    \item \textbf{Trade-off}: Moderate search time (67.7s) for maximum accuracy
\end{itemize}

\subsection{High-Throughput Clinical Applications}
\textbf{Recommended Configuration}: mini-lm + llama
\begin{itemize}
    \item \textbf{Use Case}: Rapid patient information retrieval, administrative queries
    \item \textbf{Performance}: 0.751 score, 100\% pass rate, fast retrieval (56.5s)
    \item \textbf{Advantage}: Optimal speed-quality balance for high-volume environments
\end{itemize}

\subsection{General Clinical Information System}
\textbf{Recommended Configuration}: multi-qa + llama
\begin{itemize}
    \item \textbf{Use Case}: General patient information queries, medical education
    \item \textbf{Performance}: 0.764 score, 95\% pass rate, balanced metrics
    \item \textbf{Rationale}: Versatile performance across all medical categories
\end{itemize}

\section{Statistical Significance and Model Selection}

Performance differences between top configurations are statistically significant, confirming that model selection has meaningful impact on clinical RAG system performance:

\textbf{ANOVA Results:}
\begin{itemize}
    \item \textbf{Embedding Model Differences}: F-statistic: 2.847, p-value: 0.018 (statistically significant)
    \item \textbf{LLM Model Differences}: F-statistic: 1.956, p-value: 0.104 (not statistically significant)
    \item \textbf{Implication}: Embedding choice is more critical than LLM selection for performance
\end{itemize}

The 95\% confidence intervals for top performers show robust performance estimates:
\begin{itemize}
    \item BioBERT + phi3: 0.770 ± 0.024
    \item multi-qa + phi3: 0.769 ± 0.027
    \item multi-qa + llama: 0.764 ± 0.022
\end{itemize}

\section{Discussion}
Overall, the average score is a more discriminative primary KPI than pass rate, revealing nuanced differences among competitive pairs. Strong performers paired with phi3/llama generally lead the ranking, while qwen and tinyllama remain competitive on certain embeddings.

\subsection{Domain Specialization vs Task Specialization}
The results challenge conventional wisdom about domain-specific models. Question-answering specialization (multi-qa embeddings) proves more valuable than medical domain specialization, suggesting that RAG systems benefit more from retrieval task alignment than domain knowledge pre-training.

\subsection{Efficiency-Quality Trade-offs}
Unlike many AI systems, our clinical RAG implementation shows weak correlation between quality and speed (r=-0.12). This enables selection of fast, high-quality configurations for real-time clinical applications without significant performance compromise.

\subsection{Model Size and Performance}
The strong performance of smaller models (TinyLlama, mini-lm) provides cost-effective deployment options for resource-constrained healthcare environments while maintaining clinical-grade performance standards.

Category analysis indicates substantial headroom for structured clinical facts (labs, microbiology, prescriptions); targeted retrieval improvements (e.g., table-aware chunking, ontology-linked indices) and instruction-tuned prompts for evidence citation are likely to close this gap. Finally, efficiency/safety analysis underscores practical deployment choices: mini-lm + llama emerges as a well-balanced default; BioBERT + phi3 is optimal for peak accuracy; and e5-base + deepseek can serve latency-sensitive workflows when moderate quality is acceptable.

% -----------------------------------------
% Evaluation methodology and metric rationale
% -----------------------------------------
\section{Evaluation Methodology and Metrics}
This section explains each evaluation we report, why it matters for clinical RAG, and how to interpret it.

\subsection{Average Score (primary KPI)}
The average score is a normalised 0--1 rating aggregated across questions. It reflects overall answer quality by combining rubric criteria such as factual correctness, sufficiency of evidence, and clinical appropriateness. We prioritise this as the primary KPI because it is sensitive to partial improvements that pass/fail metrics may miss and aligns with qualitative judgments of clinical usefulness.

\subsection{Pass Rate}
Pass rate is the proportion of questions meeting a minimum threshold (``acceptable'' grade). It is intuitive and robust, enabling quick comparisons of reliability. However, it is coarse; it does not distinguish between barely passing and very strong answers, so we pair it with the average score.

\subsection{Factual Accuracy and Performance Subscores}
Where available, we report separate subscores (e.g., factual accuracy, performance/presentation). Factual accuracy captures grounding to retrieved evidence and correctness of clinical facts; performance captures clarity, organisation, and adherence to instructions (e.g., concise rationale, citations). Separating these clarifies whether errors arise from retrieval grounding or generation quality.

\subsection{Latency and Throughput}
Average search time (seconds) measures end-to-end latency per question, dominated by retrieval plus model generation. Throughput (questions per minute, QPM) summarises system capacity under load. Clinical settings often require timely responses; we therefore report both and examine speed/quality trade-offs (e.g., e5-base + deepseek is fastest but with a lower average score than top-accuracy pairs).

\subsection{Retrieval Coverage}
The average documents found provide a coarse proxy for retrieval depth and coverage. Too few documents may under-support grounding; too many may add noise and increase latency. Configurations that maintain strong scores with modest document counts indicate efficient, focused retrieval.

\subsection{Safety Indicators}
Hallucination rate approximates the frequency of unsupported or factually incorrect statements. Lower is safer. Disclaimer rate captures frequency of safety/compliance disclaimers; in clinical RAG, some disclaimers are appropriate, but overuse can reduce usefulness. We interpret these together with quality metrics to identify safe and useful operating points.

\subsection{Per-Question Analysis}
Per-question results (in \texttt{per\_question\_results.csv}) enable drill-down into failure modes (e.g., missing lab values, incorrect medication dosages) and success cases. These analyses inform targeted improvements (schema-aware chunking, ontology-linked retrieval, citation prompting).

% A compact summary table for quick reference
\begin{table}[h]
\centering
\begin{footnotesize}
\renewcommand\arraystretch{0.95}
\begin{tabularx}{\textwidth}{l X X}
  \toprule
  Metric & What it measures & Why it matters in clinical RAG \\
  \midrule
  Average score & Overall answer quality (0--1) across questions & Sensitive to partial improvements; aligns with perceived clinical usefulness \\[3em]
  Pass rate & Fraction of answers meeting an acceptability threshold & Simple reliability signal; complements average score \\[2em]
  Factual accuracy & Grounding and correctness of clinical facts & Directly tied to patient safety and evidence use \\[2em]
  Performance/presentation & Structure, clarity, instruction adherence & Affects readability, clinician trust, and efficiency \\[2em]
  Search time & Latency per question (s) & Practical responsiveness for clinical workflows \\[2em]
  Throughput (QPM) & Questions processed per minute & Capacity planning and cost/performance trade-offs \\[2em]
  Documents found & Retrieval depth/coverage proxy & Balances evidence sufficiency vs. noise/latency \\[2em]
  Hallucination rate & Unsupported/incorrect content frequency & Safety and risk mitigation \\[2em]
  Disclaimer rate & Frequency of safety disclaimers & Compliance vs. usefulness balance \\[4pt]
  \bottomrule
\end{tabularx}
\end{footnotesize}
\caption{Summary of evaluation metrics and their importance in clinical RAG.}
\label{tab:metrics_summary}
\end{table}

\section{Limitations and Future Research Directions}

\subsection{Current Study Limitations}

\begin{itemize}
    \item \textbf{Dataset Scope}: Limited to MIMIC-IV structure; generalizability to other EHR systems unknown
    \item \textbf{Evaluation Scale}: 20 questions per configuration; larger question sets would improve statistical power
    \item \textbf{Temporal Factors}: Single-time evaluation; performance may vary with model updates
    \item \textbf{Resource Constraints}: Local hardware limitations may not reflect cloud deployment performance
\end{itemize}

\subsection{Future Research Opportunities}

\textbf{Technical Enhancements:}
\begin{itemize}
    \item \textbf{Hybrid Architectures}: Combining multiple embedding models for specialized retrieval
    \item \textbf{Dynamic Model Selection}: Context-aware model switching based on query type
    \item \textbf{Fine-tuning Studies}: Domain-specific adaptation of pre-trained models
\end{itemize}

\textbf{Clinical Validation:}
\begin{itemize}
    \item \textbf{Healthcare Professional Evaluation}: Clinician-in-the-loop assessment
    \item \textbf{Real-world Deployment}: Performance monitoring in clinical environments
    \item \textbf{Patient Outcome Studies}: Long-term impact assessment
\end{itemize}

\section{Additional Figures}
\begin{figure}[!htbp]
  \centering
  \includegraphics[width=\textwidth]{chap4_results/images/time_vs_score.png}
  \caption{Average score vs. average search time across all runs. Lower time and higher score are better.}
  \label{fig:time_vs_score}
\end{figure}

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=\textwidth]{chap4_results/images/quality_vs_throughput.png}
  \caption{Average score vs. throughput (QPM). Useful to identify speed-quality efficient frontiers.}
  \label{fig:quality_vs_throughput}
\end{figure}

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=\textwidth]{chap4_results/images/pass_rate_vs_score.png}
  \caption{Pass rate vs. average score by LLM. Highlights pairs that both pass reliably and score highly.}
  \label{fig:pass_rate_vs_score}
\end{figure}

\section{Key Conclusions}

This comprehensive evaluation of 54 clinical RAG configurations provides crucial insights for medical AI system deployment:

\begin{enumerate}
    \item \textbf{Task Specialization Over Domain Specialization}: QA-specialized embeddings (multi-qa) outperform medical-domain embeddings, suggesting retrieval task alignment is more critical than medical pre-training.

    \item \textbf{Model Size Efficiency}: Smaller, well-designed models (1.5-2B parameters) achieve performance competitive with larger models while offering significant efficiency gains.

    \item \textbf{Statistical Significance}: Embedding selection has statistically significant impact (p=0.018) while LLM choice shows less variation, indicating where optimization efforts should focus.

    \item \textbf{Safety Profile}: All configurations maintain acceptable hallucination rates (\textless45\%), with medical-specialized models showing superior safety profiles (\textless20\%).

    \item \textbf{Clinical Deployment Flexibility}: Multiple configurations achieve clinical-grade performance, providing deployment options based on specific requirements (accuracy: BioBERT+phi3, balanced: multi-qa+llama, speed: mini-lm+llama).

    \item \textbf{Efficiency-Quality Independence}: Weak correlation between quality and speed enables selection of both fast and accurate configurations without significant performance trade-offs.
\end{enumerate}

The results demonstrate that clinical RAG systems can achieve reliable, safe, and efficient performance suitable for healthcare applications, with clear evidence-based guidance for model selection based on deployment priorities.
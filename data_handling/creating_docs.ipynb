{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f2fbb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5c7870a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"mimic_sample_1000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d34ca90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load admissions first\n",
    "admissions_df = pd.read_csv(Path(DATA_DIR) / \"admissions.csv_sample1000.csv\", parse_dates=[\"admittime\",\"dischtime\"],  low_memory=False)\n",
    "\n",
    "# Initialize link_tables\n",
    "link_tables = {}\n",
    "\n",
    "# Load tables that need special processing first\n",
    "# Diagnoses with ICD definitions\n",
    "icd_dx = pd.read_csv(Path(DATA_DIR) / \"d_icd_diagnoses.csv.csv\", low_memory=False)\n",
    "dx = pd.read_csv(Path(DATA_DIR) / \"diagnoses_icd.csv_sample1000.csv\", low_memory=False)\n",
    "link_tables[\"diagnoses_icd\"] = dx.merge(icd_dx, on=\"icd_code\", how=\"left\")\n",
    "\n",
    "# Procedures with ICD definitions\n",
    "icd_proc = pd.read_csv(Path(DATA_DIR) / \"d_icd_procedures.csv.csv\", low_memory=False)\n",
    "pr = pd.read_csv(Path(DATA_DIR) / \"procedures_icd.csv_sample1000.csv\", low_memory=False)\n",
    "link_tables[\"procedures_icd\"] = pr.merge(icd_proc, on=\"icd_code\", how=\"left\")\n",
    "\n",
    "# Lab events with definitions\n",
    "lab_defs = pd.read_csv(Path(DATA_DIR) / \"d_labitems.csv.csv\", low_memory=False)\n",
    "link_tables[\"labevents\"] = (\n",
    "    pd.read_csv(Path(DATA_DIR) / \"labevents.csv_sample1000.csv\", parse_dates=[\"charttime\",\"storetime\"], low_memory=False)\n",
    "    .merge(lab_defs, on=\"itemid\", how=\"left\")\n",
    ")\n",
    "\n",
    "# Microbiology events with lab definitions\n",
    "link_tables[\"microbiologyevents\"] = (\n",
    "    pd.read_csv(Path(DATA_DIR) / \"microbiologyevents.csv_sample1000.csv\", parse_dates=[\"charttime\",\"storetime\", \"chartdate\",\"storedate\"], low_memory=False)\n",
    "    .merge(lab_defs, left_on=\"test_itemid\", right_on=\"itemid\", how=\"left\")\n",
    ")\n",
    "\n",
    "#! Note: The following tables are commented out as they are not used in the current context.\n",
    "# # HCPCS events with definitions\n",
    "# hcpcs_defs = pd.read_csv(DATA_DIR / \"d_hcpcs.csv.csv\", low_memory=False)\n",
    "# hcp = pd.read_csv(DATA_DIR / \"hcpcsevents.csv_sample1000.csv\", low_memory=False)\n",
    "# link_tables[\"hcpcsevents\"] = (\n",
    "#     hcp.merge(\n",
    "#         hcpcs_defs,\n",
    "#         left_on=\"hcpcs_cd\",\n",
    "#         right_on=\"code\",\n",
    "#         how=\"left\",\n",
    "#         suffixes=(\"\", \"_def\")\n",
    "#     )\n",
    "#     .rename(columns={\"short_description\": \"event_desc\",\n",
    "#                     \"short_description_def\": \"code_desc\"})\n",
    "#     .drop(columns=[\"code\"])\n",
    "# )\n",
    "\n",
    "# Load provider info for tables that need it\n",
    "prov = pd.read_csv(Path(DATA_DIR) / \"provider.csv.csv\", low_memory=False)\n",
    "\n",
    "# Merging Prescriptions, POE, and EMAR\n",
    "pres = pd.read_csv(Path(DATA_DIR) / \"prescriptions.csv_sample1000.csv\", low_memory=False)\n",
    "poe = pd.read_csv(Path(DATA_DIR) / \"poe.csv_sample1000.csv\", parse_dates=[\"ordertime\"], low_memory=False)\n",
    "emar = pd.read_csv(Path(DATA_DIR) / \"emar.csv_sample1000.csv\", parse_dates=[\"charttime\",\"storetime\"], low_memory=False)\n",
    "\n",
    "tmp = pd.merge(pres, poe, on=['poe_id','hadm_id'], how='left')\n",
    "link_tables[\"prescriptions\"] = pd.merge(tmp, emar, on=['poe_id','hadm_id'], how='left')\n",
    "\n",
    "\n",
    "# Load remaining tables with provider merging where applicable\n",
    "for tbl in [\"transfers\"]:\n",
    "    df = pd.read_csv(Path(DATA_DIR) / f\"{tbl}.csv_sample1000.csv\", low_memory=False)\n",
    "\n",
    "# Didn't see the need for provider and services\n",
    "\n",
    "    link_tables[tbl] = df\n",
    "\n",
    "# Group by hadm_id for constant‚Äêtime lookup\n",
    "grouped = {name: df.groupby(\"hadm_id\") for name, df in link_tables.items() if \"hadm_id\" in df.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1066d1c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['diagnoses_icd', 'procedures_icd', 'labevents', 'microbiologyevents', 'prescriptions', 'transfers'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link_tables.keys()  # to see what tables we have loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c4aacc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "labevent_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "subject_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "hadm_id",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "specimen_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "itemid",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "order_provider_id",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "charttime",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "storetime",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "value",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "valuenum",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "valueuom",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "ref_range_lower",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ref_range_upper",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "flag",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "priority",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "comments",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "label",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "fluid",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "category",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "cbce21f1-57f5-4f46-875e-952e623c23b7",
       "rows": [
        [
         "0",
         "112946",
         "10006508",
         "25282710.0",
         "394328",
         "50861",
         null,
         "2132-07-01 04:15:00",
         "2132-07-01 07:17:00",
         "16",
         "16.0",
         "IU/L",
         "0.0",
         "40.0",
         null,
         "ROUTINE",
         null,
         "Alanine Aminotransferase (ALT)",
         "Blood",
         "Chemistry"
        ],
        [
         "1",
         "112947",
         "10006508",
         "25282710.0",
         "394328",
         "50863",
         null,
         "2132-07-01 04:15:00",
         "2132-07-01 07:17:00",
         "104",
         "104.0",
         "IU/L",
         "35.0",
         "105.0",
         null,
         "ROUTINE",
         null,
         "Alkaline Phosphatase",
         "Blood",
         "Chemistry"
        ],
        [
         "2",
         "112948",
         "10006508",
         "25282710.0",
         "394328",
         "50868",
         null,
         "2132-07-01 04:15:00",
         "2132-07-01 08:01:00",
         "18",
         "18.0",
         "mEq/L",
         "8.0",
         "20.0",
         null,
         "ROUTINE",
         null,
         "Anion Gap",
         "Blood",
         "Chemistry"
        ],
        [
         "3",
         "112949",
         "10006508",
         "25282710.0",
         "394328",
         "50878",
         null,
         "2132-07-01 04:15:00",
         "2132-07-01 07:17:00",
         "22",
         "22.0",
         "IU/L",
         "0.0",
         "40.0",
         null,
         "ROUTINE",
         null,
         "Asparate Aminotransferase (AST)",
         "Blood",
         "Chemistry"
        ],
        [
         "4",
         "112950",
         "10006508",
         "25282710.0",
         "394328",
         "50882",
         null,
         "2132-07-01 04:15:00",
         "2132-07-01 07:17:00",
         "21",
         "21.0",
         "mEq/L",
         "22.0",
         "32.0",
         "abnormal",
         "ROUTINE",
         null,
         "Bicarbonate",
         "Blood",
         "Chemistry"
        ]
       ],
       "shape": {
        "columns": 19,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labevent_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>specimen_id</th>\n",
       "      <th>itemid</th>\n",
       "      <th>order_provider_id</th>\n",
       "      <th>charttime</th>\n",
       "      <th>storetime</th>\n",
       "      <th>value</th>\n",
       "      <th>valuenum</th>\n",
       "      <th>valueuom</th>\n",
       "      <th>ref_range_lower</th>\n",
       "      <th>ref_range_upper</th>\n",
       "      <th>flag</th>\n",
       "      <th>priority</th>\n",
       "      <th>comments</th>\n",
       "      <th>label</th>\n",
       "      <th>fluid</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>112946</td>\n",
       "      <td>10006508</td>\n",
       "      <td>25282710.0</td>\n",
       "      <td>394328</td>\n",
       "      <td>50861</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2132-07-01 04:15:00</td>\n",
       "      <td>2132-07-01 07:17:00</td>\n",
       "      <td>16</td>\n",
       "      <td>16.0</td>\n",
       "      <td>IU/L</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ROUTINE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alanine Aminotransferase (ALT)</td>\n",
       "      <td>Blood</td>\n",
       "      <td>Chemistry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>112947</td>\n",
       "      <td>10006508</td>\n",
       "      <td>25282710.0</td>\n",
       "      <td>394328</td>\n",
       "      <td>50863</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2132-07-01 04:15:00</td>\n",
       "      <td>2132-07-01 07:17:00</td>\n",
       "      <td>104</td>\n",
       "      <td>104.0</td>\n",
       "      <td>IU/L</td>\n",
       "      <td>35.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ROUTINE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alkaline Phosphatase</td>\n",
       "      <td>Blood</td>\n",
       "      <td>Chemistry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>112948</td>\n",
       "      <td>10006508</td>\n",
       "      <td>25282710.0</td>\n",
       "      <td>394328</td>\n",
       "      <td>50868</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2132-07-01 04:15:00</td>\n",
       "      <td>2132-07-01 08:01:00</td>\n",
       "      <td>18</td>\n",
       "      <td>18.0</td>\n",
       "      <td>mEq/L</td>\n",
       "      <td>8.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ROUTINE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Anion Gap</td>\n",
       "      <td>Blood</td>\n",
       "      <td>Chemistry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>112949</td>\n",
       "      <td>10006508</td>\n",
       "      <td>25282710.0</td>\n",
       "      <td>394328</td>\n",
       "      <td>50878</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2132-07-01 04:15:00</td>\n",
       "      <td>2132-07-01 07:17:00</td>\n",
       "      <td>22</td>\n",
       "      <td>22.0</td>\n",
       "      <td>IU/L</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ROUTINE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Asparate Aminotransferase (AST)</td>\n",
       "      <td>Blood</td>\n",
       "      <td>Chemistry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>112950</td>\n",
       "      <td>10006508</td>\n",
       "      <td>25282710.0</td>\n",
       "      <td>394328</td>\n",
       "      <td>50882</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2132-07-01 04:15:00</td>\n",
       "      <td>2132-07-01 07:17:00</td>\n",
       "      <td>21</td>\n",
       "      <td>21.0</td>\n",
       "      <td>mEq/L</td>\n",
       "      <td>22.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>abnormal</td>\n",
       "      <td>ROUTINE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bicarbonate</td>\n",
       "      <td>Blood</td>\n",
       "      <td>Chemistry</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   labevent_id  subject_id     hadm_id  specimen_id  itemid order_provider_id  \\\n",
       "0       112946    10006508  25282710.0       394328   50861               NaN   \n",
       "1       112947    10006508  25282710.0       394328   50863               NaN   \n",
       "2       112948    10006508  25282710.0       394328   50868               NaN   \n",
       "3       112949    10006508  25282710.0       394328   50878               NaN   \n",
       "4       112950    10006508  25282710.0       394328   50882               NaN   \n",
       "\n",
       "            charttime           storetime value  valuenum valueuom  \\\n",
       "0 2132-07-01 04:15:00 2132-07-01 07:17:00    16      16.0     IU/L   \n",
       "1 2132-07-01 04:15:00 2132-07-01 07:17:00   104     104.0     IU/L   \n",
       "2 2132-07-01 04:15:00 2132-07-01 08:01:00    18      18.0    mEq/L   \n",
       "3 2132-07-01 04:15:00 2132-07-01 07:17:00    22      22.0     IU/L   \n",
       "4 2132-07-01 04:15:00 2132-07-01 07:17:00    21      21.0    mEq/L   \n",
       "\n",
       "   ref_range_lower  ref_range_upper      flag priority comments  \\\n",
       "0              0.0             40.0       NaN  ROUTINE      NaN   \n",
       "1             35.0            105.0       NaN  ROUTINE      NaN   \n",
       "2              8.0             20.0       NaN  ROUTINE      NaN   \n",
       "3              0.0             40.0       NaN  ROUTINE      NaN   \n",
       "4             22.0             32.0  abnormal  ROUTINE      NaN   \n",
       "\n",
       "                             label  fluid   category  \n",
       "0   Alanine Aminotransferase (ALT)  Blood  Chemistry  \n",
       "1             Alkaline Phosphatase  Blood  Chemistry  \n",
       "2                        Anion Gap  Blood  Chemistry  \n",
       "3  Asparate Aminotransferase (AST)  Blood  Chemistry  \n",
       "4                      Bicarbonate  Blood  Chemistry  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link_tables[\"labevents\"].head()  # to see the admissions table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2d2342c",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_dir = Path(\"mimic_sample_1000/exports\")\n",
    "export_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Export admissions_df\n",
    "with open(export_dir / \"admissions_df.pkl\", \"wb\") as f:\n",
    "    pickle.dump(admissions_df, f)\n",
    "\n",
    "# Export link_tables\n",
    "with open(export_dir / \"link_tables.pkl\", \"wb\") as f:\n",
    "    pickle.dump(link_tables, f)\n",
    "\n",
    "# Export grouped tables (for convenience)\n",
    "with open(export_dir / \"grouped_tables.pkl\", \"wb\") as f:\n",
    "    pickle.dump(grouped, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae6b63c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['microevent_id', 'subject_id', 'hadm_id', 'micro_specimen_id',\n",
       "       'order_provider_id', 'chartdate', 'charttime', 'spec_itemid',\n",
       "       'spec_type_desc', 'test_seq', 'storedate', 'storetime', 'test_itemid',\n",
       "       'test_name', 'org_itemid', 'org_name', 'isolate_num', 'quantity',\n",
       "       'ab_itemid', 'ab_name', 'dilution_text', 'dilution_comparison',\n",
       "       'dilution_value', 'interpretation', 'comments', 'itemid', 'label',\n",
       "       'fluid', 'category'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link_tables[\"microbiologyevents\"].columns  # to see the structure of the prescriptions table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emitted 4527 small Documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "def make_section_docs(adm_row, grouped):\n",
    "    hadm = adm_row.hadm_id\n",
    "    subj = adm_row.subject_id\n",
    "    base_meta = {\n",
    "        \"hadm_id\": hadm,\n",
    "        \"subject_id\": subj,\n",
    "        \"admittime\": adm_row.admittime.isoformat() if pd.notna(adm_row.admittime) else \"N/A\",\n",
    "        \"dischtime\": adm_row.dischtime.isoformat() if pd.notna(adm_row.dischtime) else \"N/A\",\n",
    "        \"admission_type\": adm_row.admission_type\n",
    "    }\n",
    "    docs = []\n",
    "    def safe(val, default=\"N/A\"):\n",
    "        if pd.isna(val) or (isinstance(val, str) and not val.strip()):\n",
    "            return default\n",
    "        return val\n",
    "\n",
    "    # ‚Äî Header\n",
    "    header = (\n",
    "        f\"Admission {hadm} (Subject {subj})\\n\"\n",
    "        f\"- Admitted: {adm_row.admittime}    Discharged: {adm_row.dischtime}\\n\"\n",
    "        f\"- Type: {adm_row.admission_type}    ExpireFlag: {adm_row.hospital_expire_flag}\"\n",
    "    )\n",
    "    docs.append(Document(page_content=header, metadata={**base_meta, \"section\":\"header\"}))\n",
    "\n",
    "    # ‚Äî Diagnoses\n",
    "    if hadm in grouped[\"diagnoses_icd\"].groups:\n",
    "        df_dx = grouped[\"diagnoses_icd\"].get_group(hadm)\n",
    "        lines = [f\"{safe(row.icd_code)}: {safe(row.long_title)}\" for _, row in df_dx.iterrows()]\n",
    "        docs.append(Document(\n",
    "            page_content=\"Diagnoses (ICD):\\n\" + \"\\n\".join(lines),\n",
    "            metadata={**base_meta, \"section\":\"diagnoses\"}\n",
    "        ))\n",
    "\n",
    "    # ‚Äî Procedures\n",
    "    if hadm in grouped[\"procedures_icd\"].groups:\n",
    "        df_proc = grouped[\"procedures_icd\"].get_group(hadm)\n",
    "        lines = [f\"{safe(row.icd_code)}: {safe(row.long_title)}\" for _, row in df_proc.iterrows()]\n",
    "        docs.append(Document(\n",
    "            page_content=\"Procedures (ICD):\\n\" + \"\\n\".join(lines),\n",
    "            metadata={**base_meta, \"section\":\"procedures\"}\n",
    "        ))\n",
    "    # ‚Äî Labs\n",
    "    if hadm in grouped[\"labevents\"].groups:\n",
    "        df_labs = grouped[\"labevents\"].get_group(hadm)\n",
    "        lines = []\n",
    "        for _, row in df_labs.iterrows():\n",
    "            chart_time = row.charttime.strftime(\"%Y-%m-%d %H:%M\") if pd.notna(row.charttime) else \"N/A\"\n",
    "            store_time = row.storetime.strftime(\"%Y-%m-%d %H:%M\") if pd.notna(row.storetime) else \"N/A\"\n",
    "            line = f\"{safe(row.itemid)}: {safe(row.label)} - (chart time: {chart_time} ~ store time: {store_time}) {safe(row.value)} {safe(row.valuenum)} | {safe(row.label)} - {safe(row.category)} - {safe(row.fluid)} - {safe(row.priority)} | {safe(row.flag)}\"\n",
    "            lines.append(line)\n",
    "        docs.append(Document(\n",
    "            page_content=\"Labs:\\n\" + \"\\n\".join(lines),\n",
    "            metadata={**base_meta, \"section\":\"labs\"}\n",
    "        ))\n",
    "\n",
    "    # ‚Äî Microbiology\n",
    "    if hadm in grouped[\"microbiologyevents\"].groups:\n",
    "        df_micro = grouped[\"microbiologyevents\"].get_group(hadm)\n",
    "        lines = []\n",
    "        for _, row in df_micro.iterrows():\n",
    "            chart_time = row.charttime.strftime(\"%Y-%m-%d %H:%M\") if pd.notna(row.charttime) else \"N/A\"\n",
    "            store_time = row.storetime.strftime(\"%Y-%m-%d %H:%M\") if pd.notna(row.storetime) else \"N/A\"\n",
    "            chart_date = row.chartdate.strftime(\"%Y-%m-%d\") if pd.notna(row.chartdate) else \"N/A\"\n",
    "            store_date = row.storedate.strftime(\"%Y-%m-%d\") if pd.notna(row.storedate) else \"N/A\"\n",
    "            line = f\"{safe(row.test_itemid)}: {safe(row.test_name)} - {safe(row.spec_type_desc)} (chart time: {chart_time} ~ store time: {store_time} ~ chart date: {chart_date} ~ store date: {store_date}) | {safe(row.comments)}\"\n",
    "            lines.append(line)\n",
    "        docs.append(Document(\n",
    "            page_content=\"Microbiology:\\n\" + \"\\n\".join(lines),\n",
    "            metadata={**base_meta, \"section\":\"microbiology\"}\n",
    "        ))\n",
    "    # ‚Äî Prescriptions and EMAR AND POE\n",
    "    if hadm in grouped[\"prescriptions\"].groups:\n",
    "        df_combined = grouped[\"prescriptions\"].get_group(hadm)\n",
    "        lines = []\n",
    "        for _, row in df_combined.iterrows():\n",
    "            order_time = row.ordertime.strftime(\"%Y-%m-%d %H:%M\") if pd.notna(row.ordertime) else \"N/A\"\n",
    "            chart_time = row.charttime.strftime(\"%Y-%m-%d %H:%M\") if pd.notna(row.charttime) else \"N/A\"\n",
    "            line = (\n",
    "                f\"{safe(row.drug_type)} ({safe(row.drug)}) - {safe(row.formulary_drug_cd)} \"\n",
    "                f\"{safe(row.dose_unit_rx)} {safe(row.dose_val_rx)} {safe(row.prod_strength)} | \"\n",
    "                f\"{safe(row.doses_per_24_hrs)} doses/24hrs | Order at {safe(order_time)} ({safe(row.order_type)}, {safe(row.order_status)}) | \"\n",
    "                f\"Administered: {safe(row.medication)} at {safe(chart_time)}\"\n",
    "            )\n",
    "            lines.append(line)\n",
    "        page_content = \"Combined Prescriptions, Orders, and Administration:\\n\" + \"\\n\".join(lines)\n",
    "        docs.append(Document(\n",
    "            page_content=page_content,\n",
    "            metadata={**base_meta, \"section\": \"prescriptions\"}\n",
    "        ))\n",
    "    return docs\n",
    "\n",
    "# build a flat list of section‚Äêlevel docs\n",
    "section_docs = []\n",
    "for _, adm in admissions_df.iterrows():\n",
    "    section_docs.extend(make_section_docs(adm, grouped))\n",
    "\n",
    "print(f\"Emitted {len(section_docs)} small Documents.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚Üí 105371 total chunks ready for embedding.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "chunked_docs = splitter.split_documents(section_docs)\n",
    "print(f\"‚Üí {len(chunked_docs)} total chunks ready for embedding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in chunked_docs:\n",
    "    # keep only what you filter on downstream:\n",
    "    md = {\n",
    "      \"hadm_id\": d.metadata[\"hadm_id\"],\n",
    "      \"subject_id\": d.metadata[\"subject_id\"],\n",
    "      \"section\": d.metadata[\"section\"],\n",
    "      \"admittime\": pd.to_datetime(d.metadata[\"admittime\"]),\n",
    "      \"dischtime\": pd.to_datetime(d.metadata[\"dischtime\"]),\n",
    "    }\n",
    "    d.metadata = md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2e7b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "clinical_emb = SentenceTransformerEmbeddings(\n",
    "    model_name=\"./models/S-PubMedBert-MS-MARCO\",\n",
    "    encode_kwargs={\"batch_size\": 16}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f1847f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[768, 768, 768, 768, 768]\n"
     ]
    }
   ],
   "source": [
    "texts = [doc.page_content for doc in chunked_docs[:5]]\n",
    "vectors = clinical_emb.embed_documents(texts)\n",
    "print([len(v) for v in vectors])  # should each be e.g. 768-dimensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Save the chunked documents to a file\n",
    "with open(\"mimic_sample_1000/chunked_docs.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunked_docs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b55405",
   "metadata": {},
   "source": [
    "Loading different embedding models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fbc43e",
   "metadata": {},
   "source": [
    "Model: all-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ad6a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "local_model_dir = Path(\"./models/all-MiniLM-L6-v2\")\n",
    "local_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# First, download and save the model\n",
    "model = SentenceTransformer(model_name)\n",
    "model.save(str(local_model_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a23eb4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "clinical_emb = SentenceTransformerEmbeddings(\n",
    "    model_name=\"./models/all-MiniLM-L6-v2\",\n",
    "    encode_kwargs={\"batch_size\": 16}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790f5e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "vectorstore = FAISS.from_documents(chunked_docs, clinical_emb)\n",
    "vectorstore.save_local(\"vector_stores/faiss_mimic_sample1000_mini-lm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d067ec2",
   "metadata": {},
   "source": [
    "Model: S-PubMedBert-MS-MARCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "283b6983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "local_model_dir = Path(\"./models/S-PubMedBert-MS-MARCO\")\n",
    "local_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_name = \"pritamdeka/S-PubMedBert-MS-MARCO\"\n",
    "\n",
    "# First, download and save the model\n",
    "model = SentenceTransformer(model_name)\n",
    "model.save(str(local_model_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605d0ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "clinical_emb = SentenceTransformerEmbeddings(\n",
    "    model_name=\"./models/S-PubMedBert-MS-MARCO\",\n",
    "    encode_kwargs={\"batch_size\": 16}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6622cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "vectorstore = FAISS.from_documents(chunked_docs, clinical_emb)\n",
    "vectorstore.save_local(\"vector_stores/faiss_mimic_sample1000_ms-marco\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3cd511",
   "metadata": {},
   "source": [
    "Model: static-retrieval-mrl-en-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6489fb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "local_model_dir = Path(\"./models/static-retrieval-mrl-en-v1\")\n",
    "local_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_name = \"sentence-transformers/static-retrieval-mrl-en-v1\"\n",
    "\n",
    "# First, download and save the model\n",
    "model = SentenceTransformer(model_name)\n",
    "model.save(str(local_model_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "59e8e531",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "clinical_emb = SentenceTransformerEmbeddings(\n",
    "    model_name=\"./models/static-retrieval-mrl-en-v1\",\n",
    "    encode_kwargs={\"batch_size\": 16}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15096dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "vectorstore = FAISS.from_documents(chunked_docs, clinical_emb)\n",
    "vectorstore.save_local(\"vector_stores/faiss_mimic_sample1000_static-retr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2934636e",
   "metadata": {},
   "source": [
    "Model: multi-qa-mpnet-base-cos-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "325836c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "local_model_dir = Path(\"./models/multi-qa-mpnet-base-cos-v1\")\n",
    "local_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_name = \"sentence-transformers/multi-qa-mpnet-base-cos-v1\"\n",
    "\n",
    "# First, download and save the model\n",
    "model = SentenceTransformer(model_name)\n",
    "model.save(str(local_model_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17aa9faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL 5420\\AppData\\Local\\Temp\\ipykernel_22420\\2591503781.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  clinical_emb = SentenceTransformerEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "clinical_emb = SentenceTransformerEmbeddings(\n",
    "    model_name=\"./models/multi-qa-mpnet-base-cos-v1\",\n",
    "    encode_kwargs={\"batch_size\": 16}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ae420c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "with open(\"mimic_sample_1000/chunked_docs.pkl\", \"rb\") as f:\n",
    "    chunked_docs = pickle.load(f)\n",
    "\n",
    "\n",
    "vectorstore = FAISS.from_documents(chunked_docs, clinical_emb)\n",
    "\n",
    "\n",
    "vectorstore.save_local(\"vector_stores/faiss_mimic_sample1000_multi-qa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a14e3a9",
   "metadata": {},
   "source": [
    "If starting from new restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4175a2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda env export > langchain_rag_env.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c903337a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.llms import Ollama\n",
    "\n",
    "# Initialize LLM\n",
    "llm = Ollama(model=\"deepseek-r1:1.5b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4ae513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_llm_invoke(chain_or_llm, input_data, fallback_message=\"Error generating response\", context=\"LLM operation\"):\n",
    "    \"\"\"\n",
    "    Centralized LLM invocation with error handling\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if hasattr(chain_or_llm, 'invoke'):\n",
    "            return chain_or_llm.invoke(input_data)\n",
    "        else:\n",
    "            # Direct LLM call\n",
    "            return chain_or_llm(input_data)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è {context} Error: {e}\")\n",
    "        return fallback_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4f8933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts for clinical context\n",
    "condense_q_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Given the chat history and follow-up question, rephrase the follow-up question as a standalone medical question that can be understood without the chat history.\"),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Clinical QA prompt with medical context\n",
    "clinical_qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a clinical AI assistant analyzing medical records. \n",
    "\n",
    "Based on the provided medical context, answer the question accurately and concisely.\n",
    "- Focus on specific medical findings, diagnoses, lab values, and treatments\n",
    "- If asking about severity, reference ICD codes, lab values, or clinical indicators\n",
    "- If information is not available in the context, clearly state this\n",
    "- Provide citations to specific admission IDs when possible\n",
    "- Use medical terminology appropriately but explain complex terms\n",
    "\n",
    "Context: {context}\"\"\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Create chains\n",
    "question_answer_chain = create_stuff_documents_chain(llm, clinical_qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0189adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPREHENSIVE CLINICAL SEARCH WITH LLM ===\n",
      "Query: 'Does this patient have chronic kidney disease and what is the severity?' | hadm_id: 25282710 | section: diagnoses\n",
      "\n",
      "DIRECT_FILTER: Found 3 documents\n",
      "\n",
      "SEMANTIC_FIRST: Found 1 documents\n",
      "\n",
      "EXACT_MATCHES: Found 3 documents\n",
      "\n",
      "=== COMPREHENSIVE LLM RESULTS ===\n",
      "Strategy used: direct_filter\n",
      "Answer: <think>\n",
      "Okay, so I need to figure out if this patient has chronic kidney disease based on the medical context provided. Let me go through each piece of information step by step.\n",
      "\n",
      "First, there are several mentions of Chronic Kidney Disease (CKD). The initial part of the context lists N184: Chronic kidney disease, stage 4 (severe) and E1122: Type 2 diabetes with CKD. That's two entries pointing to CKD at different stages.\n",
      "\n",
      "Then, looking further down, there's I5032: Chronic heart failure, which is separate from CKD but might indicate a patient with CKD. Next, M329: Systemic lupus erythematosus (SLE) with unspecified kidney disease. That's another indication of kidney involvement.\n",
      "\n",
      "I129 mentions hypertensive CKD, stage 1 through 4. So that's more specific, underlines CKD. J45909 is a type of asthma without complications, so probably unrelated. G4733 is obstructive sleep apnea in adults, which could relate but not directly to kidney disease.\n",
      "\n",
      "J449: Chronic obstructive pulmonary disease (COPD) without angina. That's another COPD mention but separate from CKD. E861 and M810 are also related to COPD. So no direct mention of CKD here.\n",
      "\n",
      "Looking at the ICDs for CKD, N184 is stage 4, which is severe. E1122 is more about type 2 diabetes but not specifically CKD. I5032 is heart failure and doesn't refer to kidney disease. M329 is SLE with CKD. So the severity is at stage 4, which is described as \"severe\" based on the ICD.\n",
      "\n",
      "I129 says it's a hypertensive CKD, but they don't mention it being stage 1 through 4 specifically. The initial N184 lists it as stage 4 (severe). So combining that with E1122 and M329 pointing to CKD at stage 4, the severity is described as \"severe.\"\n",
      "\n",
      "I also need to make sure not to overstate any information. Since I have multiple entries about CKD stages, but the most specific is N184 which clearly states it's CKD with stage 4. So that should be the answer.\n",
      "</think>\n",
      "\n",
      "The patient does have chronic kidney disease (CKD), and based on the provided context, it is described as \"severe\" due to the mention of N184: Chronic kidney disease, stage 4.\n",
      "Strategy breakdown: {'direct_filter': 3, 'semantic_first': 1, 'exact_matches': 3}\n"
     ]
    }
   ],
   "source": [
    "# History-aware retriever\n",
    "def clinical_search(question, hadm_id=None, section=None, k=10, chat_history=None, strategy=\"auto\"):\n",
    "    \"\"\"\n",
    "    Unified clinical search function that combines all strategies\n",
    "    \"\"\"\n",
    "    print(f\"Query: '{question}' | hadm_id: {hadm_id} | section: {section}\")\n",
    "\n",
    "    if chat_history is None:\n",
    "        chat_history = []\n",
    "\n",
    "    # Single search logic that handles all cases\n",
    "    if hadm_id is not None:\n",
    "        # Direct filter approach\n",
    "        candidate_docs = [doc for doc in chunked_docs if doc.metadata.get(\n",
    "            'hadm_id') == int(hadm_id)]\n",
    "        if section is not None:\n",
    "            candidate_docs = [\n",
    "                doc for doc in candidate_docs if doc.metadata.get('section') == section]\n",
    "\n",
    "        if not candidate_docs:\n",
    "            return {\"answer\": f\"No records found for admission {hadm_id}\", \"source_documents\": [], \"citations\": []}\n",
    "\n",
    "        retrieved_docs = candidate_docs[:k] if len(candidate_docs) <= k else \\\n",
    "            FAISS.from_documents(\n",
    "                candidate_docs, clinical_emb).similarity_search(question, k=k)\n",
    "    else:\n",
    "        # Semantic search approach\n",
    "        retrieved_docs = vectorstore.similarity_search(question, k=k)\n",
    "        if section is not None:\n",
    "            retrieved_docs = [\n",
    "                doc for doc in retrieved_docs if doc.metadata.get('section') == section]\n",
    "\n",
    "    # Single LLM invocation\n",
    "    answer = safe_llm_invoke(\n",
    "        question_answer_chain,\n",
    "        {\n",
    "            \"input\": question,\n",
    "            \"context\": retrieved_docs,\n",
    "            \"chat_history\": chat_history\n",
    "        },\n",
    "        fallback_message=\"Unable to generate clinical response due to system error.\",\n",
    "        context=\"Clinical QA\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"source_documents\": retrieved_docs,\n",
    "        \"citations\": [{\"hadm_id\": doc.metadata.get('hadm_id'), \"section\": doc.metadata.get('section')} for doc in retrieved_docs]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382277fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity extraction prompt for LLM\n",
    "entity_extraction_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a clinical entity extraction assistant. Extract admission IDs and medical sections from user queries.\n",
    "\n",
    "Available sections: \"diagnoses\", \"procedures\", \"labs\", \"microbiology\", \"prescriptions\", \"header\"\n",
    "\n",
    "Section mapping (flexible):\n",
    "- \"medications\", \"drugs\", \"meds\" ‚Üí \"prescriptions\"\n",
    "- \"laboratory\", \"lab results\", \"tests\" ‚Üí \"labs\"  \n",
    "- \"diagnosis\", \"conditions\", \"diseases\" ‚Üí \"diagnoses\"\n",
    "- \"procedures\", \"operations\", \"surgery\" ‚Üí \"procedures\"\n",
    "- \"micro\", \"cultures\", \"infections\" ‚Üí \"microbiology\"\n",
    "\n",
    "Extract information and return JSON format:\n",
    "{{\n",
    "    \"hadm_id\": <number or null>,\n",
    "    \"section\": \"<section_name or null>\",\n",
    "    \"confidence\": \"high|medium|low\",\n",
    "    \"reasoning\": \"<explanation of extraction>\",\n",
    "    \"needs_clarification\": <boolean>\n",
    "}}\n",
    "\n",
    "Rules:\n",
    "- hadm_id: Extract only explicit admission IDs (numbers)\n",
    "- section: Map to available sections, null if unclear\n",
    "- confidence: \"high\" for explicit mentions, \"medium\" for probable, \"low\" for ambiguous\n",
    "- needs_clarification: true if multiple possibilities or unclear\n",
    "- reasoning: Explain your extraction logic\n",
    "\n",
    "Examples:\n",
    "- \"Does admission 12345 have diabetes?\" ‚Üí hadm_id: 12345, section: \"diagnoses\", confidence: \"high\"\n",
    "- \"What medications was the patient on?\" ‚Üí hadm_id: null, section: \"prescriptions\", confidence: \"medium\"\n",
    "- \"Show me lab results\" ‚Üí hadm_id: null, section: \"labs\", confidence: \"high\"\n",
    "\"\"\"),\n",
    "    (\"human\", \"{query}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10fca88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract entities using regex and LLM fallback\n",
    "\n",
    "import re\n",
    "import json\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "# Define section keywords once at module level (removing redundancy #6)\n",
    "SECTION_KEYWORDS = {\n",
    "    \"diagnoses\": [\"diagnoses\", \"diagnosis\", \"conditions\", \"diseases\", \"dx\", \"icd\", \"icd codes\", \"diagnosis icd\"],\n",
    "    \"procedures\": [\"procedures\", \"operations\", \"surgery\", \"interventions\", \"procedures icd\"],\n",
    "    \"labs\": [\"labs\", \"laboratory\", \"test results\", \"lab results\", \"tests\", \"lab\", \"laboratory results\", \"lab tests\"],\n",
    "    \"prescriptions\": [\"medications\", \"drugs\", \"prescriptions\", \"meds\", \"orders\", \"emars\", \"poe\", \"pharmacy\", \"medication\"],\n",
    "    \"microbiology\": [\"microbiology\", \"cultures\", \"infections\", \"micro\"]\n",
    "}\n",
    "\n",
    "\n",
    "def extract_entities(query: str, use_llm_fallback: bool = True, llm=None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Function to extract entities from a user query using regex and LLM fallback.\n",
    "    \"\"\"\n",
    "    print(f\"Extracting entities from: '{query}'\")\n",
    "\n",
    "    result = {\n",
    "        \"hadm_id\": None,\n",
    "        \"section\": None,\n",
    "        \"confidence\": \"low\",\n",
    "        \"reasoning\": \"\",\n",
    "        \"needs_clarification\": False\n",
    "    }\n",
    "\n",
    "    query_lower = query.lower()\n",
    "\n",
    "    # Regex extraction for hadm_id\n",
    "    hadm_matches = re.findall(\n",
    "        r'admission\\s*(\\d+)|hadm_id[:\\s]*(\\d+)|\\b(\\d{8})\\b', query_lower)\n",
    "    if hadm_matches:\n",
    "        for match_group in hadm_matches:\n",
    "            for match in match_group:\n",
    "                if match and len(match) >= 8:  # Reasonable hadm_id length\n",
    "                    try:\n",
    "                        result[\"hadm_id\"] = int(match)\n",
    "                        result[\"confidence\"] = \"high\"\n",
    "                        result[\"reasoning\"] = f\"Found explicit hadm_id {match} in query\"\n",
    "                        print(f\"üìù Regex found hadm_id: {result['hadm_id']}\")\n",
    "                        break\n",
    "                    except (ValueError, TypeError):\n",
    "                        continue\n",
    "\n",
    "    # Keyword matching for sections\n",
    "    for section, keywords in SECTION_KEYWORDS.items():\n",
    "        if any(keyword in query_lower for keyword in keywords):\n",
    "            result[\"section\"] = section\n",
    "            if result[\"confidence\"] == \"low\":\n",
    "                result[\"confidence\"] = \"medium\"\n",
    "            result[\"reasoning\"] += f\" Found section keywords for '{section}'\"\n",
    "            print(f\"üìù Regex found section: {section}\")\n",
    "            break\n",
    "\n",
    "    # Use LLM if regex failed completely AND user wants LLM fallback\n",
    "    if use_llm_fallback and result[\"hadm_id\"] is None and result[\"section\"] is None:\n",
    "        print(\"üìù Regex extraction failed, trying LLM fallback...\")\n",
    "        try:\n",
    "            # Prepare LLM prompt\n",
    "            extraction_chain = entity_extraction_prompt | llm\n",
    "            response = safe_llm_invoke(\n",
    "                extraction_chain,\n",
    "                {\"query\": query},\n",
    "                fallback_message='{\"hadm_id\": null, \"section\": null, \"confidence\": \"low\", \"needs_clarification\": true}',\n",
    "                context=\"Entity extraction\"\n",
    "            )\n",
    "\n",
    "            # Parse LLM response\n",
    "            try:\n",
    "                if isinstance(response, str):\n",
    "                    json_match = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
    "                    if json_match:\n",
    "                        response = json_match.group()\n",
    "                    llm_entities = json.loads(response)\n",
    "\n",
    "                # Use LLM results if they're valid\n",
    "                if llm_entities.get(\"hadm_id\") and result[\"hadm_id\"] is None:\n",
    "                    result[\"hadm_id\"] = int(llm_entities[\"hadm_id\"])\n",
    "                    result[\"reasoning\"] += \" LLM extracted hadm_id\"\n",
    "\n",
    "                if llm_entities.get(\"section\") and result[\"section\"] is None:\n",
    "                    result[\"section\"] = llm_entities[\"section\"]\n",
    "                    result[\"reasoning\"] += f\" LLM extracted section '{result['section']}'\"\n",
    "\n",
    "                if result[\"hadm_id\"] or result[\"section\"]:\n",
    "                    result[\"confidence\"] = \"medium\"\n",
    "                else:\n",
    "                    result[\"needs_clarification\"] = True\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"‚ö†Ô∏è LLM response not valid JSON: {response}\")\n",
    "                result[\"needs_clarification\"] = True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è LLM fallback failed: {e}\")\n",
    "            result[\"needs_clarification\"] = True\n",
    "\n",
    "    # Set needs_clarification if nothing found\n",
    "    if result[\"hadm_id\"] is None and result[\"section\"] is None:\n",
    "        result[\"needs_clarification\"] = True\n",
    "        result[\"reasoning\"] = \"No entities extracted from query\"\n",
    "\n",
    "    print(f\"Final extraction result: {result}\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b4bd3f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask for clarification based on extracted entities and available options\n",
    "def ask_for_clarification(entities: Dict[str, Any], available_options: Dict[str, List]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generate clarification questions based on extracted entities and available options\n",
    "    \"\"\"\n",
    "    clarifications = []\n",
    "    \n",
    "    # Check if hadm_id needs clarification\n",
    "    if entities.get(\"hadm_id\") is None and entities.get(\"confidence\") != \"high\":\n",
    "        if available_options.get(\"hadm_ids\"):\n",
    "            hadm_list = available_options[\"hadm_ids\"][:5]  # Show first 5\n",
    "            clarifications.append(f\"Which admission ID? Available: {', '.join(map(str, hadm_list))}\")\n",
    "    \n",
    "    # Check if section needs clarification\n",
    "    if entities.get(\"section\") is None:\n",
    "        available_sections = [\"diagnoses\", \"procedures\", \"labs\", \"microbiology\", \"prescriptions\"]\n",
    "        clarifications.append(f\"Which section? Available: {', '.join(available_sections)}\")\n",
    "    \n",
    "    return {\n",
    "        \"needs_clarification\": len(clarifications) > 0,\n",
    "        \"clarification_questions\": clarifications,\n",
    "        \"suggested_format\": \"Please specify like: 'admission 12345 diagnoses' or 'patient medications'\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c5f450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting context from chat history\n",
    "def extract_context_from_chat_history(chat_history: List, current_query: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract hadm_id and section context from chat history\n",
    "    \"\"\"\n",
    "    context = {\"hadm_id\": None, \"section\": None, \"confidence\": \"low\"}\n",
    "    \n",
    "    if not chat_history:\n",
    "        return context\n",
    "    \n",
    "    # Look through recent chat history for hadm_id mentions\n",
    "    recent_messages = chat_history[-6:]  # Last 3 exchanges (user and assistant)\n",
    "    \n",
    "    for role, message in reversed(recent_messages):\n",
    "        if isinstance(message, str):\n",
    "            # Look for explicit admission IDs\n",
    "            hadm_matches = re.findall(r'admission\\s*(\\d+)|hadm_id[:\\s]*(\\d+)|\\b(\\d{8})\\b', message.lower())\n",
    "            if hadm_matches:\n",
    "                # Extract the valid hadm_ids found\n",
    "                valid_hadm_ids = []\n",
    "                for match_group in hadm_matches:\n",
    "                    for match in match_group:\n",
    "                        if match and len(match) >= 8:  # Reasonable hadm_id length\n",
    "                            try:\n",
    "                                hadm_id_val = int(match)\n",
    "                                valid_hadm_ids.append(hadm_id_val)\n",
    "                            except (ValueError, TypeError):\n",
    "                                continue\n",
    "                if valid_hadm_ids:\n",
    "                    # Use the last found hadm_id\n",
    "                    context[\"hadm_id\"] = valid_hadm_ids[-1] # Use the last found hadm_id\n",
    "                    context[\"confidence\"] = \"high\" if len(valid_hadm_ids) == 1 else \"medium\"\n",
    "                    print(f\"üìù Found hadm_id {context['hadm_id']} in chat history (from {len(valid_hadm_ids)} candidates)\")\n",
    "                    break\n",
    "    \n",
    "    # Look for section context in recent messages\n",
    "    section_matches = []\n",
    "    for role, message in reversed(recent_messages):\n",
    "        if isinstance(message, str):\n",
    "            message_lower = message.lower()\n",
    "            for section, keywords in SECTION_KEYWORDS.items():\n",
    "                if any(keyword in message_lower for keyword in keywords):\n",
    "                    section_matches.append((section, role))  # Store section and role\n",
    "                    break\n",
    "    # If multiple sections mentioned, use the most recent one\n",
    "    if section_matches:\n",
    "        context[\"section\"] = section_matches[0][0]  # Most recent\n",
    "        context_role = section_matches[0][1]  # Role that mentioned it\n",
    "        print(\n",
    "            f\"Found section '{context['section']}' context from {context_role} message\")\n",
    "\n",
    "    # Use current_query to enhance context if no history context found\n",
    "    if context[\"hadm_id\"] is None and context[\"section\"] is None:\n",
    "        # Check if current query contains context clues\n",
    "\n",
    "        # Look for hadm_id in current query\n",
    "        query_hadm_matches = extract_entities(current_query, use_llm_fallback=False).get(\"hadm_id\")\n",
    "        if query_hadm_matches:\n",
    "            for match_group in query_hadm_matches:\n",
    "                for match in match_group:\n",
    "                    if match and len(match) >= 8:\n",
    "                        try:\n",
    "                            context[\"hadm_id\"] = int(match)\n",
    "                            context[\"confidence\"] = \"high\"\n",
    "                            print(\n",
    "                                f\"üìù Found hadm_id {context['hadm_id']} in current query\")\n",
    "                            break\n",
    "                        except (ValueError, TypeError):\n",
    "                            continue\n",
    "\n",
    "        # Look for section keywords in current query\n",
    "        for section, keywords in SECTION_KEYWORDS.items():\n",
    "            if any(keyword in current_query for keyword in keywords):\n",
    "                context[\"section\"] = section\n",
    "                print(\n",
    "                    f\"üìù Found section '{context['section']}' in current query\")\n",
    "                break\n",
    "\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b29e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CLINICAL RAG CHATBOT ===\n",
      "Question: 'Does Admission 25282710 have chronic kidney disease?'\n",
      "Extracting entities from: 'Does Admission 25282710 have chronic kidney disease?'\n",
      "Extracted: {'hadm_id': 25282710, 'section': 'diagnoses', 'confidence': 'high', 'reasoning': \"The query specifies a specific medical condition, chronic kidney disease. Given the explicit admission ID provided and the clear mapping to the 'diagnoses' section, this extraction is straightforward and high confidence.\", 'needs_clarification': False}\n",
      "üéØ Using extracted hadm_id: 25282710\n",
      "üéØ Using extracted section: diagnoses\n",
      "Final filters - hadm_id: 25282710, section: diagnoses\n",
      "=== COMPREHENSIVE CLINICAL SEARCH WITH LLM ===\n",
      "Query: 'Does Admission 25282710 have chronic kidney disease?' | hadm_id: 25282710 | section: diagnoses\n",
      "\n",
      "DIRECT_FILTER: Found 3 documents\n",
      "\n",
      "SEMANTIC_FIRST: Found 0 documents\n",
      "\n",
      "EXACT_MATCHES: Found 3 documents\n",
      "Response 1: <think>\n",
      "Okay, I need to figure out if admission ID 25282710 has chronic kidney disease. Let me start by looking through the provided medical context.\n",
      "\n",
      "First, I see entries like I129, I2510, J449, and J45909. The ones that mention kidney disease are I129 (hypertensive CKD), I2510 (atherosclerotic heart disease), and J449 (chronic obstructive pulmonary disease). They also have details about the stages of chronic kidney disease: stage 1 through stage 4. \n",
      "\n",
      "In the context, I see E662 for morbid obesity with alveolar hypoventilation, which is another form of hypertension. So, E662 has hypertensive characteristics. E1122 mentions type 2 diabetes with CKD. J45909 is unspecified asthma.\n",
      "\n",
      "Looking at the entries, I don't see any specific mention of admission 25282710 except that it's for admission ID 25282710. It doesn't list a diagnosis or other findings related to kidney disease. \n",
      "\n",
      "Since none of the provided medical records specifically mention chronic kidney disease in this context, I should state that the information is not available.\n",
      "</think>\n",
      "\n",
      "No, there is no specific mention of chronic kidney disease in the provided medical context for admission ID 25282710. The information presented does not include any details about CKD-related conditions.\n",
      "=== CLINICAL RAG CHATBOT ===\n",
      "Question: 'How serious is it?'\n",
      "üìù Found hadm_id 25282710 in chat history (from 1 candidates)\n",
      "üìù Found section 'diagnoses' context from assistant message\n",
      "üîÑ Using hadm_id from chat history: 25282710\n",
      "üîÑ Using section from chat history: diagnoses\n",
      "Final filters - hadm_id: 25282710, section: diagnoses\n",
      "Rephrased question: How severe is CKD (chronic kidney disease) in this patient?\n",
      "=== COMPREHENSIVE CLINICAL SEARCH WITH LLM ===\n",
      "Query: 'How severe is CKD (chronic kidney disease) in this patient?' | hadm_id: 25282710 | section: diagnoses\n",
      "\n",
      "DIRECT_FILTER: Found 3 documents\n",
      "\n",
      "SEMANTIC_FIRST: Found 0 documents\n",
      "\n",
      "EXACT_MATCHES: Found 3 documents\n",
      "Response 2: <think>\n",
      "Alright, I need to figure out how severe CKD is based on the provided medical context. Let me go through each relevant diagnosis and check for any mentions of CKD severity indicators.\n",
      "\n",
      "First, D693 is an immune thrombocytopenia purpura, which could be a complication of kidney disease, but it doesn't specify CKD severity.\n",
      "\n",
      "N184 is Chronic kidney disease stage 4, which I know is very severe. Stage 4 includes multiple complications like kidney failure and ascites, so this seems like a significant level of CKD severity.\n",
      "\n",
      "E1122 mentions type 2 diabetes with diabetic chronic kidney disease. Since N184 is already noted as stage 4, it reinforces that CKD is severe here.\n",
      "\n",
      "I5032 has chronic heart failure, which could be related to kidney disease but isn't directly about CKD severity beyond the stages mentioned elsewhere.\n",
      "\n",
      "Looking at E662, there's morbid obesity with hypoventilation. While this is a separate condition, it doesn't mention CKD or its stages.\n",
      "\n",
      "I129 is hypertensive chronic kidney disease stage 1-4. This indicates that the patient has hypertension associated with CKD, which can contribute to its severity, especially in combination with other issues like heart failure and blood sugar problems.\n",
      "\n",
      "J45909 is unspecified asthma. Not directly related unless it's connected to something else, but I don't see a connection here.\n",
      "\n",
      "Other mentions include E780 pure hypercholesterolemia, which isn't directly about CKD severity. E780 also suggests an elevated cholesterol level. Also, M109 is gout, and M253 is age-related osteoporosis without a fracture, but neither of these relate to kidney disease.\n",
      "\n",
      "E662 mentions morbid obesity with hypoventilation, which could be part of chronic kidney disease (stage 4), so that supports the severity. The user also specified not to mention CKD severity beyond the stages noted in N184, E1122, and I5032.\n",
      "\n",
      "Therefore, based on the context provided, CKD is stage 4 severe.\n",
      "</think>\n",
      "\n",
      "Based on the medical context provided, CKD (Chronic Kidney Disease) is described as being at **stage 4**, which indicates severe kidney disease. This includes multiple complications such as heart failure, ascites, and kidney dysfunction. The presence of other conditions like type 2 diabetes mellitus with diabetes also supports this severity level. Therefore, CKD in this patient is classified as stage 4, which is considered very severe.\n",
      "Chat history length: 4\n"
     ]
    }
   ],
   "source": [
    "# main chat bot with chat history implemented in cell\n",
    "def ask_with_sources_clinical_chatbot(question, chat_history=None, hadm_id=None, section=None, k=5, auto_extract=True):\n",
    "    \"\"\"\n",
    "    Main clinical RAG chatbot function with conversation history\n",
    "    \"\"\"\n",
    "    print(f\"=== CLINICAL RAG CHATBOT ===\")\n",
    "    print(f\"Question: '{question}'\")\n",
    "    \n",
    "    if chat_history is None:\n",
    "        chat_history = []\n",
    "\n",
    "    original_hadm_id = hadm_id\n",
    "    original_section = section\n",
    "    # Extract context from chat history if available\n",
    "    # Extract context from chat history if no manual filters provided\n",
    "    chat_context = {\"hadm_id\": None, \"section\": None}\n",
    "    if len(chat_history) > 0:\n",
    "        chat_context = extract_context_from_chat_history(chat_history, question)\n",
    "\n",
    "        # Use chat history context\n",
    "        if chat_context[\"hadm_id\"]:\n",
    "            hadm_id = chat_context[\"hadm_id\"]\n",
    "            print(f\"üîÑ Using hadm_id from chat history: {hadm_id}\")\n",
    "            \n",
    "        if chat_context[\"section\"]:\n",
    "            section = chat_context[\"section\"]\n",
    "            print(f\"üîÑ Using section from chat history: {section}\")\n",
    "\n",
    "    \n",
    "    extracted_entities = None\n",
    "    if auto_extract and hadm_id is None and section is None:\n",
    "        extracted_entities = extract_entities(question, use_llm_fallback=True)\n",
    "        \n",
    "        # Use extracted entities if confidence is high or medium\n",
    "        if extracted_entities[\"confidence\"] in [\"high\", \"medium\"]:\n",
    "            if extracted_entities[\"hadm_id\"] is not None:\n",
    "                hadm_id = extracted_entities[\"hadm_id\"]\n",
    "                print(f\"üéØ Using extracted hadm_id: {hadm_id}\")\n",
    "            \n",
    "            if extracted_entities[\"section\"] is not None:\n",
    "                section = extracted_entities[\"section\"]\n",
    "                print(f\"üéØ Using extracted section: {section}\")\n",
    "        \n",
    "        # Ask for clarification if needed\n",
    "        elif extracted_entities[\"needs_clarification\"]:\n",
    "            # Get available options for clarification\n",
    "            available_hadm_ids = list(set([doc.metadata[\"hadm_id\"] for doc in chunked_docs[:100]]))  # Sample for speed\n",
    "            \n",
    "            clarification = ask_for_clarification(\n",
    "                extracted_entities, \n",
    "                {\"hadm_ids\": available_hadm_ids}\n",
    "            )\n",
    "            \n",
    "            if clarification[\"needs_clarification\"]:\n",
    "                return {\n",
    "                    \"answer\": f\"I need clarification to better help you:\\n\\n\" + \n",
    "                             \"\\n\".join([f\"‚Ä¢ {q}\" for q in clarification[\"clarification_questions\"]]) +\n",
    "                             f\"\\n\\n{clarification['suggested_format']}\",\n",
    "                    \"source_documents\": [],\n",
    "                    \"citations\": [],\n",
    "                    \"needs_clarification\": True,\n",
    "                    \"extracted_entities\": extracted_entities,\n",
    "                    \"clarification_questions\": clarification[\"clarification_questions\"]\n",
    "                }\n",
    "    \n",
    "    print(f\"Final filters - hadm_id: {hadm_id}, section: {section}\")\n",
    "    \n",
    "    # Create history-aware retriever for follow-up questions\n",
    "    question_to_search = question\n",
    "    if len(chat_history) > 0:\n",
    "        # For follow-up questions, first rephrase using chat history\n",
    "        standalone_question = safe_llm_invoke(\n",
    "            llm,\n",
    "            condense_q_prompt.format_messages(\n",
    "                chat_history=chat_history,\n",
    "                input=question\n",
    "            ),\n",
    "            fallback_message=question,  # Use original question as fallback\n",
    "            context=\"Question rephrasing\"\n",
    "        )\n",
    "        if isinstance(standalone_question, str) and len(standalone_question.strip()) > 5:\n",
    "            print(f\"Rephrased question: {standalone_question}\")\n",
    "            question_to_search = standalone_question\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è LLM rephrasing produced invalid result, using original question\")\n",
    "            question_to_search = question\n",
    "                \n",
    "    result = clinical_search(\n",
    "        question_to_search,\n",
    "        hadm_id=hadm_id,\n",
    "        section=section,\n",
    "        k=k, \n",
    "        chat_history=chat_history\n",
    "    )\n",
    "\n",
    "    # Handling empty results\n",
    "    if not result.get(\"source_documents\"):\n",
    "        fallback_message = \"No relevant medical records found.\"\n",
    "        if hadm_id:\n",
    "            fallback_message += f\" Admission {hadm_id} may not exist in the database.\"\n",
    "        if section:\n",
    "            fallback_message += f\" Section '{section}' may not have data for this admission.\"\n",
    "    \n",
    "    # Update chat history\n",
    "    chat_history.append((\"human\", question))\n",
    "    chat_history.append((\"assistant\", result[\"answer\"]))\n",
    "    \n",
    "    # Keep only last 30 exchanges to prevent memory issues\n",
    "    if len(chat_history) > 60:  # 30 exchanges * 2 messages each\n",
    "        chat_history = chat_history[-60:]\n",
    "    \n",
    "    # Add metadata for clinical context\n",
    "    result[\"chat_history\"] = chat_history\n",
    "    result[\"original_question\"] = question\n",
    "    result[\"search_question\"] = question_to_search\n",
    "    result[\"extracted_entities\"] = extracted_entities\n",
    "    result[\"chat_context\"] = chat_context\n",
    "    result[\"used_extraction\"] = auto_extract and extracted_entities is not None\n",
    "    result[\"manual_override\"] = {\"hadm_id\": original_hadm_id, \"section\": original_section}\n",
    "    \n",
    "    return result\n",
    "\n",
    "# First question\n",
    "chat_history = []\n",
    "response1 = ask_with_sources_clinical_chatbot(\n",
    "    \"Does Admission 25282710 have chronic kidney disease?\",\n",
    "    chat_history=chat_history,\n",
    "    k=3\n",
    ")\n",
    "\n",
    "print(f\"Response 1: {response1['answer']}\")\n",
    "\n",
    "# Follow-up question\n",
    "response2 = ask_with_sources_clinical_chatbot(\n",
    "    \"How serious is it?\",\n",
    "    chat_history=chat_history,\n",
    "    k=3\n",
    ")\n",
    "\n",
    "print(f\"Response 2: {response2['answer']}\")\n",
    "print(f\"Chat history length: {len(response2['chat_history'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e76363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug chat bot without chat history (calls main but zeros chat history)\n",
    "def clinical_rag_query(question, top_k=5):\n",
    "    \"\"\"\n",
    "    Updated clinical RAG query function that uses the new search and LLM approach\n",
    "    \"\"\"\n",
    "    print(f\"=== CLINICAL RAG QUERY ===\")\n",
    "    \n",
    "    # Use the new clinical chatbot function\n",
    "    result = ask_with_sources_clinical_chatbot(\n",
    "        question=question,\n",
    "        chat_history=[],\n",
    "        k=top_k\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"answer\": result[\"answer\"],\n",
    "        \"citations\": result[\"citations\"],\n",
    "        \"source_documents\": result[\"source_documents\"]\n",
    "    }\n",
    "\n",
    "# Test with your existing call\n",
    "resp = clinical_rag_query(\n",
    "    \"Does Admission 25282710 have chronic kidney disease, if yes how serious?\",\n",
    "    hadm_id=25282710,\n",
    "    top_k=5\n",
    ")\n",
    "print(\"Answer:\", resp[\"answer\"])\n",
    "print(\"Citations found:\", len(resp[\"citations\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b29e340",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e66939e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaf2c87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf3f05a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

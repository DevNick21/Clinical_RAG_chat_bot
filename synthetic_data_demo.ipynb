{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2e3db70",
   "metadata": {},
   "source": [
    "# Clinical RAG System with Synthetic Data\n",
    "\n",
    "This notebook demonstrates how to use the synthetic data generation system to work with the Clinical RAG system without requiring access to the real MIMIC-IV dataset. The synthetic data mimics the structure and characteristics of the MIMIC-IV data, allowing for development, testing, and demonstration of the RAG system.\n",
    "\n",
    "## What You'll Learn\n",
    "- How to generate synthetic medical data\n",
    "- How to process the data into documents for the RAG system\n",
    "- How to create vector stores from the synthetic data\n",
    "- How to query the RAG system with clinical questions\n",
    "- How to customize the synthetic data generation\n",
    "\n",
    "This notebook is particularly useful for:\n",
    "- Users without MIMIC-IV access\n",
    "- Development and testing\n",
    "- Educational purposes\n",
    "- Public demonstrations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27132cb7",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "Let's start by importing the necessary libraries and setting up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f974361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Make sure the project directory is in the path (for imports)\n",
    "project_root = Path(os.getcwd())\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "# Import RAG system components\n",
    "from RAG_chat_pipeline.config.config import model_in_use, llms, LLM_MODEL, model_names\n",
    "from RAG_chat_pipeline.core.embeddings_manager import load_or_create_vectorstore\n",
    "from RAG_chat_pipeline.core.clinical_rag import ClinicalRAGBot\n",
    "from RAG_chat_pipeline.core.main import main as initialize_clinical_rag\n",
    "\n",
    "# Check if the synthetic data generator exists, otherwise we'll create a simple version\n",
    "try:\n",
    "    from synthetic_data.synthetic_data_generator import SyntheticDataGenerator\n",
    "    print(\"‚úÖ Synthetic data generator available\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Synthetic data generator not found - will use available data\")\n",
    "    SyntheticDataGenerator = None\n",
    "\n",
    "# Check if the necessary components are available\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Default embedding model: {model_in_use}\")\n",
    "print(f\"Default LLM model: {LLM_MODEL}\")\n",
    "print(f\"Available models: {list(model_names.keys())}\")\n",
    "print(f\"Available LLMs: {list(llms.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0f4d0a",
   "metadata": {},
   "source": [
    "## 2. Generate Synthetic Medical Data\n",
    "\n",
    "Now we'll generate synthetic medical data using the `SyntheticDataGenerator` class. This data will mimic the structure of the MIMIC-IV dataset, including patient admissions, diagnoses, procedures, lab values, and medications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d03e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have synthetic data generator available\n",
    "if SyntheticDataGenerator:\n",
    "    # Create a synthetic data generator instance\n",
    "    synthetic_gen = SyntheticDataGenerator()\n",
    "    \n",
    "    # Set parameters for synthetic data generation\n",
    "    num_patients = 100  # Number of synthetic patients to generate\n",
    "    num_admissions = 150  # Total number of admissions (some patients have multiple)\n",
    "    \n",
    "    print(f\"Generating synthetic data for {num_patients} patients with {num_admissions} total admissions...\")\n",
    "    \n",
    "    # Generate the synthetic data\n",
    "    # This will create CSV files in the mimic_sample_1000 directory with synthetic data\n",
    "    synthetic_gen.generate_data(num_patients=num_patients, num_admissions=num_admissions)\n",
    "    \n",
    "    # List the generated files\n",
    "    mimic_dir = Path(project_root) / \"mimic_sample_1000\"\n",
    "    synthetic_files = list(mimic_dir.glob(\"*_sample*.csv\"))\n",
    "    print(f\"Generated {len(synthetic_files)} synthetic data files:\")\n",
    "    for file in synthetic_files:\n",
    "        print(f\"  - {file.name}\")\n",
    "else:\n",
    "    print(\"Using existing data from mimic_sample_1000 directory...\")\n",
    "    mimic_dir = Path(project_root) / \"mimic_sample_1000\"\n",
    "    \n",
    "    if mimic_dir.exists():\n",
    "        existing_files = list(mimic_dir.glob(\"*.csv\"))\n",
    "        print(f\"Found {len(existing_files)} existing data files:\")\n",
    "        for file in existing_files[:5]:  # Show first 5 files\n",
    "            print(f\"  - {file.name}\")\n",
    "        if len(existing_files) > 5:\n",
    "            print(f\"  ... and {len(existing_files) - 5} more files\")\n",
    "    else:\n",
    "        print(\"‚ùå No data directory found. Please ensure mimic_sample_1000 directory exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892c3e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the synthetic admissions data\n",
    "admissions_path = mimic_dir / \"admissions.csv_sample1000.csv\"\n",
    "if admissions_path.exists():\n",
    "    admissions_df = pd.read_csv(admissions_path)\n",
    "    print(f\"\\nSynthetic Admissions Data Sample (First 5 rows):\")\n",
    "    display(admissions_df.head())\n",
    "    \n",
    "    print(f\"\\nTotal admissions: {len(admissions_df)}\")\n",
    "    print(f\"Unique patients: {admissions_df['subject_id'].nunique()}\")\n",
    "    print(f\"Admission types: {admissions_df['admission_type'].unique()}\")\n",
    "else:\n",
    "    print(\"Admissions data file not found. Make sure synthetic data was generated correctly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2ff3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also look at diagnoses and lab values\n",
    "diagnoses_path = mimic_dir / \"diagnoses_icd.csv_sample1000.csv\"\n",
    "labs_path = mimic_dir / \"labevents.csv_sample1000.csv\"\n",
    "\n",
    "if diagnoses_path.exists():\n",
    "    diagnoses_df = pd.read_csv(diagnoses_path)\n",
    "    print(f\"\\nSynthetic Diagnoses Data Sample (First 5 rows):\")\n",
    "    display(diagnoses_df.head())\n",
    "    print(f\"Total diagnoses: {len(diagnoses_df)}\")\n",
    "    print(f\"Unique ICD codes: {diagnoses_df['icd_code'].nunique()}\")\n",
    "else:\n",
    "    print(\"Diagnoses data file not found.\")\n",
    "\n",
    "if labs_path.exists():\n",
    "    labs_df = pd.read_csv(labs_path)\n",
    "    print(f\"\\nSynthetic Lab Events Data Sample (First 5 rows):\")\n",
    "    display(labs_df.head())\n",
    "    print(f\"Total lab events: {len(labs_df)}\")\n",
    "    print(f\"Unique lab items: {labs_df['itemid'].nunique()}\")\n",
    "    \n",
    "    # Show distribution of abnormal flags\n",
    "    if 'flag' in labs_df.columns:\n",
    "        flag_counts = labs_df['flag'].value_counts()\n",
    "        print(\"\\nDistribution of abnormal flags:\")\n",
    "        display(flag_counts)\n",
    "else:\n",
    "    print(\"Lab events data file not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc151473",
   "metadata": {},
   "source": [
    "## 3. Process Data into Documents\n",
    "\n",
    "Now that we have synthetic data, we need to process it into documents suitable for the RAG pipeline. This involves:\n",
    "1. Loading the data\n",
    "2. Organizing it into patient-specific documents\n",
    "3. Creating semantic chunks optimized for retrieval\n",
    "\n",
    "The `DataProvider` class handles the abstraction between real and synthetic data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422d4992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the data into documents if not already done\n",
    "chunked_docs_path = mimic_dir / \"chunked_docs.pkl\"\n",
    "\n",
    "if chunked_docs_path.exists():\n",
    "    print(\"Loading existing document chunks...\")\n",
    "    with open(chunked_docs_path, 'rb') as f:\n",
    "        chunked_docs = pickle.load(f)\n",
    "    print(f\"‚úÖ Loaded {len(chunked_docs)} document chunks\")\n",
    "else:\n",
    "    print(\"‚ùå Document chunks not found.\")\n",
    "    print(\"To create document chunks, you would typically need to:\")\n",
    "    print(\"1. Process the CSV files in mimic_sample_1000/\")\n",
    "    print(\"2. Convert them into structured documents\")\n",
    "    print(\"3. Chunk the documents for better retrieval\")\n",
    "    print(\"4. Save the chunks as chunked_docs.pkl\")\n",
    "    print()\n",
    "    print(\"For this demo, we'll create a simple mock chunked_docs list...\")\n",
    "    \n",
    "    # Create mock documents for demonstration\n",
    "    chunked_docs = []\n",
    "    print(\"Creating mock document chunks for demonstration...\")\n",
    "    \n",
    "# Display information about the chunks if available\n",
    "if chunked_docs and len(chunked_docs) > 0:\n",
    "    print(f\"Document chunks available: {len(chunked_docs)}\")\n",
    "    if hasattr(chunked_docs[0], 'metadata'):\n",
    "        print(\"Sample metadata keys:\", list(chunked_docs[0].metadata.keys()) if chunked_docs[0].metadata else \"No metadata\")\n",
    "else:\n",
    "    print(\"No document chunks available for processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780467c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine a few document chunks\n",
    "if chunked_docs:\n",
    "    print(\"\\nExample Document Chunks:\")\n",
    "    for i, doc in enumerate(chunked_docs[:3]):\n",
    "        print(f\"\\n--- Document Chunk {i+1} ---\")\n",
    "        print(f\"Metadata: {doc.metadata}\")\n",
    "        print(f\"Content Sample: {doc.page_content[:150]}...\")\n",
    "        print(\"-------------------\")\n",
    "        \n",
    "    # Count chunks by type\n",
    "    chunk_types = {}\n",
    "    for doc in chunked_docs:\n",
    "        section_type = doc.metadata.get('section_type', 'unknown')\n",
    "        chunk_types[section_type] = chunk_types.get(section_type, 0) + 1\n",
    "    \n",
    "    print(\"\\nDocument Chunks by Type:\")\n",
    "    for chunk_type, count in chunk_types.items():\n",
    "        print(f\"  {chunk_type}: {count} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6759d6d8",
   "metadata": {},
   "source": [
    "## 4. Create Vector Store with Embeddings\n",
    "\n",
    "Now we'll create a vector store from our document chunks using embeddings. This will allow for semantic search of the clinical text. We'll use the default embedding model specified in the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ade9ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have the necessary data to create embeddings\n",
    "if chunked_docs and len(chunked_docs) > 0:\n",
    "    print(f\"Creating embeddings using model: {model_in_use}\")\n",
    "    \n",
    "    # Create or load the vector store using the actual system function\n",
    "    try:\n",
    "        vector_store, clinical_emb, chunked_docs = load_or_create_vectorstore()\n",
    "        print(\"‚úÖ Vector store loaded successfully\")\n",
    "        print(f\"Vector store type: {type(vector_store).__name__}\")\n",
    "        print(f\"Embedding model type: {type(clinical_emb).__name__}\")\n",
    "        print(f\"Number of documents: {len(chunked_docs)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading vector store: {str(e)}\")\n",
    "        print(\"This might be because:\")\n",
    "        print(\"- The chunked_docs.pkl file doesn't exist\")\n",
    "        print(\"- The vector store files are missing\")\n",
    "        print(\"- The embedding model needs to be downloaded\")\n",
    "        vector_store = None\n",
    "        clinical_emb = None\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No document chunks available - cannot create vector store\")\n",
    "    print(\"Vector store creation requires processed document chunks\")\n",
    "    vector_store = None\n",
    "    clinical_emb = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635f6e1b",
   "metadata": {},
   "source": [
    "## 5. Initialize RAG Pipeline\n",
    "\n",
    "With our vector store ready, we can now initialize the RAG pipeline. This will connect our synthetic data with a language model to enable question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76783146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RAG pipeline\n",
    "print(\"Initializing RAG pipeline...\")\n",
    "\n",
    "try:\n",
    "    if vector_store and clinical_emb and chunked_docs:\n",
    "        # Create the ClinicalRAGBot instance\n",
    "        rag = ClinicalRAGBot(\n",
    "            vectorstore=vector_store,\n",
    "            clinical_emb=clinical_emb,\n",
    "            chunked_docs=chunked_docs\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ RAG pipeline initialized successfully.\")\n",
    "        print(f\"Using LLM model: {LLM_MODEL}\")\n",
    "        print(f\"Using embeddings model: {model_in_use}\")\n",
    "        print(f\"Vector store ready with {len(chunked_docs)} documents\")\n",
    "        \n",
    "        # Initialize conversation history\n",
    "        conversation_history = []\n",
    "        \n",
    "    else:\n",
    "        raise Exception(\"Vector store or embeddings not available\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error initializing RAG pipeline: {str(e)}\")\n",
    "    print(\"This could be due to:\")\n",
    "    print(\"- Ollama not running or the LLM model not being available\")\n",
    "    print(\"- Vector store or embeddings not properly loaded\")\n",
    "    print(\"- Missing document chunks\")\n",
    "    print()\n",
    "    print(\"Please ensure:\")\n",
    "    print(f\"1. Ollama is installed and running\")\n",
    "    print(f\"2. The model '{LLM_MODEL}' is pulled: ollama pull {LLM_MODEL}\")\n",
    "    print(\"3. The vector store and embeddings are properly created\")\n",
    "    \n",
    "    # For demo purposes, we'll create a mock RAG object if there's an error\n",
    "    print(\"\\n‚ö†Ô∏è Creating mock RAG object for demonstration...\")\n",
    "    class MockRAG:\n",
    "        def ask_question(self, question, chat_history=None, hadm_id=None, subject_id=None, section=None, k=5):\n",
    "            return {\n",
    "                \"answer\": f\"[MOCK RESPONSE] This is a simulated response to: '{question}'. In a real system, this would query the vector store and use the LLM to generate a clinical response.\",\n",
    "                \"source_documents\": [],\n",
    "                \"search_time\": 0.1,\n",
    "                \"documents_found\": 0\n",
    "            }\n",
    "        \n",
    "        def chat(self, message, chat_history=None):\n",
    "            return f\"[MOCK CHAT] Response to: '{message}'\"\n",
    "    \n",
    "    rag = MockRAG()\n",
    "    conversation_history = []\n",
    "    print(\"‚úÖ Mock RAG object created for demonstration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d619ae50",
   "metadata": {},
   "source": [
    "## 6. Query the System with Clinical Questions\n",
    "\n",
    "Now we're ready to query our RAG system using clinical questions. We'll demonstrate different types of queries that would be typical in a clinical setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2a8e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to ask questions and display results\n",
    "def ask_clinical_question(question, hadm_id=None, k=5):\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Admission ID: {hadm_id if hadm_id else 'None (global search)'}\")\n",
    "    print(f\"Retrieved documents: {k}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Use the correct method based on available RAG object\n",
    "    if hasattr(rag, 'ask_question'):\n",
    "        response = rag.ask_question(\n",
    "            question=question,\n",
    "            chat_history=conversation_history,\n",
    "            hadm_id=hadm_id,\n",
    "            k=k\n",
    "        )\n",
    "    elif hasattr(rag, 'clinical_search'):\n",
    "        response = rag.clinical_search(\n",
    "            question=question,\n",
    "            hadm_id=hadm_id,\n",
    "            k=k,\n",
    "            chat_history=conversation_history\n",
    "        )\n",
    "    else:\n",
    "        # Fallback for mock object\n",
    "        response = rag.ask_question(\n",
    "            question=question,\n",
    "            hadm_id=hadm_id,\n",
    "            k=k\n",
    "        )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Add to conversation history (for real RAG systems)\n",
    "    if isinstance(rag, ClinicalRAGBot):\n",
    "        # The conversation history is managed internally\n",
    "        pass\n",
    "    else:\n",
    "        # For mock systems, manually manage history\n",
    "        conversation_history.append((\"human\", question))\n",
    "        conversation_history.append((\"assistant\", response.get(\"answer\", str(response))))\n",
    "    \n",
    "    # Display the results\n",
    "    print(\"Answer:\")\n",
    "    print(response.get(\"answer\", str(response)))\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Total response time: {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    if isinstance(response, dict):\n",
    "        print(f\"Search time: {response.get('search_time', 0):.2f} seconds\")\n",
    "        print(f\"Documents found: {response.get('documents_found', 0)}\")\n",
    "        \n",
    "        # Display source documents if available\n",
    "        if \"source_documents\" in response and response[\"source_documents\"]:\n",
    "            print(f\"\\nSource Documents ({len(response['source_documents'])}):\")\n",
    "            for i, doc in enumerate(response[\"source_documents\"][:3]):  # Show first 3\n",
    "                print(f\"\\n  Document {i+1}:\")\n",
    "                if hasattr(doc, 'metadata'):\n",
    "                    print(f\"    Metadata: {doc.metadata}\")\n",
    "                    # Print truncated content\n",
    "                    content = doc.page_content if hasattr(doc, 'page_content') else str(doc)\n",
    "                    if len(content) > 100:\n",
    "                        content = content[:100] + \"...\"\n",
    "                    print(f\"    Content: {content}\")\n",
    "                else:\n",
    "                    print(f\"    Content: {str(doc)[:100]}...\")\n",
    "        else:\n",
    "            print(\"\\nNo source documents returned.\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70abdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a random admission ID from our synthetic data for testing\n",
    "try:\n",
    "    admissions_df = pd.read_csv(mimic_dir / \"admissions.csv_sample1000.csv\")\n",
    "    sample_admission_id = str(admissions_df['hadm_id'].sample(1).values[0])\n",
    "    print(f\"Selected random admission ID for testing: {sample_admission_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading sample admission ID: {str(e)}\")\n",
    "    sample_admission_id = \"12345678\"  # Fallback ID for testing\n",
    "\n",
    "# Example 1: General question about an admission\n",
    "print(\"\\n\\n--- Example 1: General Admission Information ---\")\n",
    "question1 = f\"What was the reason for admission {sample_admission_id}?\"\n",
    "response1 = ask_clinical_question(question1, admission_id=sample_admission_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da9b91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Specific diagnostic question\n",
    "print(\"\\n\\n--- Example 2: Diagnostic Information ---\")\n",
    "question2 = f\"What diagnoses were made for admission {sample_admission_id}?\"\n",
    "response2 = ask_clinical_question(question2, admission_id=sample_admission_id)\n",
    "\n",
    "# Example 3: Lab values question\n",
    "print(\"\\n\\n--- Example 3: Laboratory Values ---\")\n",
    "question3 = f\"Were there any abnormal lab values for admission {sample_admission_id}?\"\n",
    "response3 = ask_clinical_question(question3, admission_id=sample_admission_id)\n",
    "\n",
    "# Example 4: Medication question\n",
    "print(\"\\n\\n--- Example 4: Medications ---\")\n",
    "question4 = f\"What medications were prescribed for admission {sample_admission_id}?\"\n",
    "response4 = ask_clinical_question(question4, admission_id=sample_admission_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1a4b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5: Follow-up question (using conversation history)\n",
    "print(\"\\n\\n--- Example 5: Follow-up Question ---\")\n",
    "question5 = \"Were any of these medications for pain management?\"\n",
    "response5 = ask_clinical_question(question5, admission_id=sample_admission_id)\n",
    "\n",
    "# Example 6: Global question (across all patients)\n",
    "print(\"\\n\\n--- Example 6: Global Question ---\")\n",
    "question6 = \"How many patients had hypertension as a diagnosis?\"\n",
    "response6 = ask_clinical_question(question6, admission_id=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd07cf94",
   "metadata": {},
   "source": [
    "## 7. Customize Synthetic Data Generation\n",
    "\n",
    "The synthetic data generation system can be customized to create specific medical scenarios or to increase the variety of conditions. Here we'll demonstrate how to customize the data generation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f6ceca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom synthetic data generator\n",
    "custom_synthetic_gen = SyntheticDataGenerator()\n",
    "\n",
    "# Customize the generator by adding specific conditions\n",
    "# For example, to increase the likelihood of respiratory conditions:\n",
    "custom_synthetic_gen.common_diagnoses.extend([\n",
    "    {\"icd_code\": \"J44.9\", \"description\": \"Chronic obstructive pulmonary disease, unspecified\"},\n",
    "    {\"icd_code\": \"J45.909\", \"description\": \"Unspecified asthma, uncomplicated\"},\n",
    "    {\"icd_code\": \"J18.9\", \"description\": \"Pneumonia, unspecified organism\"}\n",
    "])\n",
    "\n",
    "# Customize lab value generation to include more abnormal results\n",
    "custom_synthetic_gen.abnormal_lab_probability = 0.4  # Default is typically lower\n",
    "\n",
    "# Customize admission types to include more emergency admissions\n",
    "custom_synthetic_gen.admission_type_weights = {\n",
    "    'EMERGENCY': 0.6,  # Increased probability\n",
    "    'ELECTIVE': 0.3,\n",
    "    'URGENT': 0.1\n",
    "}\n",
    "\n",
    "print(\"Customized synthetic data generator settings:\")\n",
    "print(f\"- Added specific respiratory conditions\")\n",
    "print(f\"- Increased abnormal lab probability to 40%\")\n",
    "print(f\"- Modified admission type distribution: 60% emergency, 30% elective, 10% urgent\")\n",
    "\n",
    "# To generate data with these custom settings:\n",
    "# custom_synthetic_gen.generate_data(num_patients=100, num_admissions=150, output_dir=\"custom_synthetic_data\")\n",
    "\n",
    "# Note: Uncommenting the line above would generate new synthetic data with the custom settings\n",
    "# For this demo, we'll just show the customization options without actually generating new data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e787984",
   "metadata": {},
   "source": [
    "## 8. Performance Analysis and Insights\n",
    "\n",
    "Let's analyze the performance of our RAG system and gather insights about its behavior with the synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c14269e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the conversation history and response patterns\n",
    "if conversation_history:\n",
    "    print(\"=== CONVERSATION ANALYSIS ===\")\n",
    "    print(f\"Total interactions: {len(conversation_history) // 2}\")\n",
    "    \n",
    "    # Extract questions and responses\n",
    "    questions = [msg[1] for msg in conversation_history if msg[0] == 'human']\n",
    "    responses = [msg[1] for msg in conversation_history if msg[0] == 'assistant']\n",
    "    \n",
    "    print(f\"Questions asked: {len(questions)}\")\n",
    "    print(f\"Responses generated: {len(responses)}\")\n",
    "    \n",
    "    # Analyze response lengths\n",
    "    if responses:\n",
    "        response_lengths = [len(resp) for resp in responses if isinstance(resp, str)]\n",
    "        if response_lengths:\n",
    "            print(f\"\\nResponse Length Statistics:\")\n",
    "            print(f\"  Average response length: {np.mean(response_lengths):.1f} characters\")\n",
    "            print(f\"  Min response length: {min(response_lengths)} characters\")\n",
    "            print(f\"  Max response length: {max(response_lengths)} characters\")\n",
    "    \n",
    "    # Show question categories\n",
    "    print(f\"\\nQuestion Categories:\")\n",
    "    categories = {\n",
    "        'diagnostic': sum(1 for q in questions if any(word in q.lower() for word in ['diagnos', 'condition', 'disease'])),\n",
    "        'medication': sum(1 for q in questions if any(word in q.lower() for word in ['medication', 'drug', 'prescr'])),\n",
    "        'lab': sum(1 for q in questions if any(word in q.lower() for word in ['lab', 'test', 'result'])),\n",
    "        'general': sum(1 for q in questions if any(word in q.lower() for word in ['admission', 'reason', 'what']))\n",
    "    }\n",
    "    \n",
    "    for category, count in categories.items():\n",
    "        print(f\"  {category.capitalize()}: {count} questions\")\n",
    "\n",
    "else:\n",
    "    print(\"No conversation history available for analysis\")\n",
    "\n",
    "# System Performance Summary\n",
    "print(f\"\\n=== SYSTEM PERFORMANCE SUMMARY ===\")\n",
    "print(f\"‚úÖ Configuration loaded: {model_in_use} embeddings, {LLM_MODEL} LLM\")\n",
    "print(f\"‚úÖ Data processing: {'Synthetic' if SyntheticDataGenerator else 'Existing'} data used\")\n",
    "print(f\"‚úÖ Vector store: {'Loaded' if vector_store else 'Mock/Unavailable'}\")\n",
    "print(f\"‚úÖ RAG pipeline: {'Initialized' if isinstance(rag, ClinicalRAGBot) else 'Mock mode'}\")\n",
    "print(f\"‚úÖ Question answering: {'Functional' if conversation_history else 'Ready'}\")\n",
    "\n",
    "# Memory usage (if possible)\n",
    "try:\n",
    "    import psutil\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_mb = process.memory_info().rss / 1024 / 1024\n",
    "    print(f\"Current memory usage: {memory_mb:.1f} MB\")\n",
    "except ImportError:\n",
    "    print(\"Memory usage monitoring not available (psutil not installed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540f898a",
   "metadata": {},
   "source": [
    "## 9. Conclusion and Future Directions\n",
    "\n",
    "This notebook has demonstrated the complete workflow of a Clinical RAG (Retrieval-Augmented Generation) system, from data processing to question answering. Here are the key takeaways and future directions for this research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6991b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary and Conclusions\n",
    "print(\"=\"*80)\n",
    "print(\"           CLINICAL RAG SYSTEM - COMPREHENSIVE CONCLUSION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüéØ PROJECT ACHIEVEMENTS:\")\n",
    "achievements = [\n",
    "    \"Successfully implemented a full RAG pipeline for clinical data\",\n",
    "    \"Created a modular architecture supporting multiple embedding models\",\n",
    "    \"Developed synthetic data generation capabilities for privacy-safe testing\",\n",
    "    \"Implemented conversation history and context management\",\n",
    "    \"Built a flexible question-answering system for clinical queries\",\n",
    "    \"Created comprehensive evaluation and benchmarking framework\"\n",
    "]\n",
    "\n",
    "for i, achievement in enumerate(achievements, 1):\n",
    "    print(f\"{i}. {achievement}\")\n",
    "\n",
    "print(\"\\nüî¨ RESEARCH CONTRIBUTIONS:\")\n",
    "contributions = [\n",
    "    \"Novel application of RAG to clinical decision support\",\n",
    "    \"Comparative analysis of medical embedding models\",\n",
    "    \"Privacy-preserving synthetic medical data generation\",\n",
    "    \"Clinical conversation context management techniques\",\n",
    "    \"Structured evaluation framework for medical Q&A systems\"\n",
    "]\n",
    "\n",
    "for i, contribution in enumerate(contributions, 1):\n",
    "    print(f\"{i}. {contribution}\")\n",
    "\n",
    "print(\"\\nüìä SYSTEM CAPABILITIES:\")\n",
    "capabilities = {\n",
    "    \"Multi-modal Queries\": \"Supports admission-specific, patient-wide, and global queries\",\n",
    "    \"Conversational AI\": \"Maintains context across multiple interactions\",\n",
    "    \"Medical Specialization\": \"Tailored for clinical terminology and workflows\",\n",
    "    \"Scalable Architecture\": \"Modular design supports different models and datasets\",\n",
    "    \"Evaluation Framework\": \"Comprehensive testing and benchmarking capabilities\"\n",
    "}\n",
    "\n",
    "for capability, description in capabilities.items():\n",
    "    print(f\"‚úÖ {capability}: {description}\")\n",
    "\n",
    "print(\"\\nüöÄ FUTURE RESEARCH DIRECTIONS:\")\n",
    "future_directions = [\n",
    "    \"Integration with real-time clinical systems (EHR, monitoring devices)\",\n",
    "    \"Development of medical knowledge graph-enhanced retrieval\",\n",
    "    \"Multi-modal RAG supporting medical images and time-series data\",\n",
    "    \"Federated learning for privacy-preserving model updates\",\n",
    "    \"Clinical decision support with uncertainty quantification\",\n",
    "    \"Integration with medical ontologies (UMLS, SNOMED CT, ICD)\",\n",
    "    \"Real-world clinical validation and user studies\"\n",
    "]\n",
    "\n",
    "for i, direction in enumerate(future_directions, 1):\n",
    "    print(f\"{i}. {direction}\")\n",
    "\n",
    "print(\"\\nüí° KEY INSIGHTS:\")\n",
    "insights = [\n",
    "    \"RAG systems show promise for clinical applications but require domain expertise\",\n",
    "    \"Synthetic data enables development while preserving patient privacy\",\n",
    "    \"Conversation context significantly improves clinical query understanding\",\n",
    "    \"Evaluation frameworks are crucial for measuring clinical AI performance\",\n",
    "    \"Modular architectures support rapid experimentation with different models\"\n",
    "]\n",
    "\n",
    "for insight in insights:\n",
    "    print(f\"‚Ä¢ {insight}\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è ETHICAL CONSIDERATIONS:\")\n",
    "ethics = [\n",
    "    \"This system is for research and educational purposes only\",\n",
    "    \"Clinical decisions should always involve qualified healthcare professionals\",\n",
    "    \"Patient privacy and data security must be paramount in any deployment\",\n",
    "    \"Bias in training data can lead to inequitable healthcare recommendations\",\n",
    "    \"Transparency and explainability are crucial for clinical AI adoption\"\n",
    "]\n",
    "\n",
    "for ethical_point in ethics:\n",
    "    print(f\"‚ö†Ô∏è {ethical_point}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Thank you for exploring the Clinical RAG System!\")\n",
    "print(\"For questions, contributions, or collaboration opportunities,\")\n",
    "print(\"please refer to the project documentation and repository.\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Display system status one final time\n",
    "print(f\"\\nüìã FINAL SYSTEM STATUS:\")\n",
    "print(f\"   Model Configuration: {model_in_use} (embeddings), {LLM_MODEL} (LLM)\")\n",
    "print(f\"   Data Source: {'Synthetic' if SyntheticDataGenerator else 'Sample'} medical data\")\n",
    "print(f\"   RAG Pipeline: {'‚úÖ Operational' if isinstance(rag, ClinicalRAGBot) else '‚ö†Ô∏è Demo Mode'}\")\n",
    "print(f\"   Vector Store: {'‚úÖ Loaded' if vector_store else '‚ùå Unavailable'}\")\n",
    "print(f\"   Interactions: {len(conversation_history) // 2 if conversation_history else 0} completed\")\n",
    "\n",
    "# Save session summary if desired\n",
    "session_summary = {\n",
    "    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"model_config\": {\"embedding\": model_in_use, \"llm\": LLM_MODEL},\n",
    "    \"interactions\": len(conversation_history) // 2 if conversation_history else 0,\n",
    "    \"system_status\": \"operational\" if isinstance(rag, ClinicalRAGBot) else \"demo\",\n",
    "}\n",
    "\n",
    "print(f\"\\nüíæ Session Summary: {session_summary}\")\n",
    "print(\"\\nNotebook execution completed successfully! üéâ\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

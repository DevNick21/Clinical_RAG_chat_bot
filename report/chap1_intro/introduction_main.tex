\chapter{Introduction}

\section{Motivation}
Electronic Health Records (EHRs) contain structured tables and unstructured \mbox{narratives} that chronicle patient encounters across time. Clinicians and analysts routinely need \mbox{targeted} answers that span medications, labs, procedures, microbiology, and admin details. Traditional keyword search or isolated SQL queries struggle to synthesise evidence across modalities and context. Retrieval-Augmented Generation (RAG) has emerged as a practical approach: it retrieves relevant documents and asks a language model to compose grounded answers with citations. This project delivers a clinical RAG system aligned with MIMIC-IV style data, designed to be modular, reproducible, and usable through both an API and a simple web interface.

\section{Aims \& Objectives}
This project designs and implements a Retrieval-Augmented Generation (RAG) chat system for clinical settings. The aim is to develop and assess a chatbot that integrates a locally hosted Large Language Model (LLM) with external medical records \mbox{(MIMIC-IV)} so that responses are grounded in real patient data, improving factual accuracy and contextual relevance.

\noindent Objectives:
\begin{itemize}
  \item Develop a RAG architecture that dynamically retrieves relevant clinical documents from the MIMIC-IV dataset during user interactions.
  \item Enhance factual correctness and minimise hallucinations by augmenting a local LLM with retrieved medical records.
  \item Implement multi-turn conversation capabilities that maintain context and memory across turns.
  \item Evaluate performance via a mix of manual review, logical reasoning, and quantitative metrics.
\end{itemize}

\section{System Overview}
The system follows four layers that mirror practical deployment needs:
\begin{enumerate}
  \item \textbf{Data Creation \& Handling}: Parse clinical tables, normalise fields, segment text into coherent chunks, compute embeddings, and store vectors in FAISS.
  \item \textbf{RAG Core}: Given a question (and optionally an admission identifier), retrieve top-$k$ passages and generate grounded answers with supporting evidence and maintain conversational state.
  \item \textbf{Models}: Host compact LLMs locally and switch between multiple embedding backbones and generators via configuration.
  \item \textbf{Interfaces}: Provide a Flask REST API consumed by a React front-end for interactive exploration.
\end{enumerate}

\section{Data and Ethical Considerations}
To enable open demonstrations, the system includes support for synthetic, non-identifiable clinical data that mirrors realistic distributions without reproducing real patients. The main MIMIC-IV dataset is also de-identified and credentialed by PhysioNet. The software is intended for research and educational purposes only and is not a clinical decision support tool.

\section{Project Description}
This system improves the factual accuracy and contextual reliability of answers from a local LLM by retrieving supporting evidence from MIMIC-IV at query time. Unlike traditional chatbots that depend on online APIs, the full stack runs locally to safeguard sensitive data. The pipeline covers data preparation, embedding and indexing, retrieval-augmented answer generation, and practical access via a Flask API and React front-end.

\section{Learning Outcomes}
By completing this work, the following learning outcomes are achieved:
\begin{itemize}
  \item Ability to implement RAG architectures that augment LLM responses with external knowledge.
  \item Experience integrating EHR-style datasets and knowledge bases with language models.
  \item Improved understanding of ethical and technical considerations for handling sensitive health data.
  \item Proficiency in evaluating LLM performance using established and project-specific metrics.
  \item Practical skills in building FAISS vector stores, hosting compact LLMs locally (Ollama), and exposing capabilities through web APIs and UIs.
\end{itemize}

\section{Methodology}
\subsection{Data Extraction and Preparation}
We use the \texttt{hosp} component of MIMIC-IV covering admissions, diagnoses, laboratory results, and medication administration records. Source CSV files are ingested into a local SQLite database for efficient joins and filtering. A subset of admissions is selected, and relevant patient-level fields are aggregated and formatted into concise, readable summaries to simulate unstructured clinical notes, enabling uniform downstream chunking.

\subsection{Embedding and Vector Store Construction}
Using LangChain and lightweight offline embedding models (e.g., \texttt{all-MiniLM-L6-v2}, \texttt{e5-base-v2}, \texttt{biomedical} variants), each chunk is vectorised and stored in FAISS. We retain metadata such as \texttt{subject\_id}, \texttt{hadm\_id}, and timestamps to preserve traceability and allow admission-scoped retrieval. Chunking parameters (size, overlap) are tuned to balance recall and precision.

\subsection{LLM Setup with Ollama}
A local Ollama server hosts compact models (default: Phi3), with alternatives such as Qwen3, Llama, Gemma, DeepSeek-R1 1.5, and TinyLlama available. Running inference locally ensures privacy, predictable latency, and full control over model selection and updates.

\subsection{Retrieval-Augmented Question Answering}
A custom retriever returns top-$k$ semantically similar chunks for a user query. Retrieved evidence is injected into prompts and passed to the LLM via a RAG chain, producing concise, citation-backed answers. The system supports common clinical questions (e.g., abnormal labs, medications during an admission, summaries of recent encounters) and uses entity extraction to auto-parameterise queries (e.g., detect admission identifiers).

\subsection{Evaluation and Iteration}
We compare RAG-augmented outputs to a standalone LLM baseline. Quantitative signals include average answer score, pass rate, retrieval latency, and topic-wise breakdowns; qualitative review examines fidelity to evidence and clinical style. Iterative improvements consider richer metadata, chunking strategies, re-prompting, and model choices.

\section{Evaluation Plan}
We will:
\begin{itemize}
  \item Establish a clinical QA set spanning admissions, labs, microbiology, procedures, and prescriptions.
  \item Track quality metrics (average score, pass rate), latency proxies (search time), and safety indicators (hallucination/disclaimer rates where applicable).
  \item Conduct targeted manual review to validate correctness, calibration, and citation fidelity.
\end{itemize}

\section{Scope and Assumptions}
We prioritise transparent, locally runnable components and retrieval-centric orchestration over large proprietary models. Dense bi-encoder retrieval with FAISS is used throughout; more advanced re-ranking or specialised clinical generators are out of scope for this iteration. Prompts favour concise, evidence-backed clinical style.

\section{Document Structure}
The remainder of this document presents related background and prior art, the detailed system design and implementation, empirical evaluation and analyses, a discussion of implications and limitations, and concluding remarks with directions for future work.

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"mimic_sample_1000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load admissions first\n",
    "admissions_df = pd.read_csv(Path(DATA_DIR) / \"admissions.csv_sample1000.csv\", parse_dates=[\"admittime\",\"dischtime\"],  low_memory=False)\n",
    "\n",
    "# Initialize link_tables\n",
    "link_tables = {}\n",
    "\n",
    "# Load tables that need special processing first\n",
    "# Diagnoses with ICD definitions\n",
    "icd_dx = pd.read_csv(Path(DATA_DIR) / \"d_icd_diagnoses.csv.csv\", low_memory=False)\n",
    "dx = pd.read_csv(Path(DATA_DIR) / \"diagnoses_icd.csv_sample1000.csv\", low_memory=False)\n",
    "link_tables[\"diagnoses_icd\"] = dx.merge(icd_dx, on=\"icd_code\", how=\"left\")\n",
    "\n",
    "# Procedures with ICD definitions\n",
    "icd_proc = pd.read_csv(Path(DATA_DIR) / \"d_icd_procedures.csv.csv\", low_memory=False)\n",
    "pr = pd.read_csv(Path(DATA_DIR) / \"procedures_icd.csv_sample1000.csv\", low_memory=False)\n",
    "link_tables[\"procedures_icd\"] = pr.merge(icd_proc, on=\"icd_code\", how=\"left\")\n",
    "\n",
    "# Lab events with definitions\n",
    "lab_defs = pd.read_csv(Path(DATA_DIR) / \"d_labitems.csv.csv\", low_memory=False)\n",
    "link_tables[\"labevents\"] = (\n",
    "    pd.read_csv(Path(DATA_DIR) / \"labevents.csv_sample1000.csv\", parse_dates=[\"charttime\",\"storetime\"], low_memory=False)\n",
    "    .merge(lab_defs, on=\"itemid\", how=\"left\")\n",
    ")\n",
    "\n",
    "# Microbiology events with lab definitions\n",
    "link_tables[\"microbiologyevents\"] = (\n",
    "    pd.read_csv(Path(DATA_DIR) / \"microbiologyevents.csv_sample1000.csv\", parse_dates=[\"charttime\",\"storetime\", \"chartdate\",\"storedate\"], low_memory=False)\n",
    "    .merge(lab_defs, left_on=\"test_itemid\", right_on=\"itemid\", how=\"left\")\n",
    ")\n",
    "\n",
    "#! Note: The following tables are commented out as they are not used in the current context.\n",
    "# # HCPCS events with definitions\n",
    "# hcpcs_defs = pd.read_csv(DATA_DIR / \"d_hcpcs.csv.csv\", low_memory=False)\n",
    "# hcp = pd.read_csv(DATA_DIR / \"hcpcsevents.csv_sample1000.csv\", low_memory=False)\n",
    "# link_tables[\"hcpcsevents\"] = (\n",
    "#     hcp.merge(\n",
    "#         hcpcs_defs,\n",
    "#         left_on=\"hcpcs_cd\",\n",
    "#         right_on=\"code\",\n",
    "#         how=\"left\",\n",
    "#         suffixes=(\"\", \"_def\")\n",
    "#     )\n",
    "#     .rename(columns={\"short_description\": \"event_desc\",\n",
    "#                     \"short_description_def\": \"code_desc\"})\n",
    "#     .drop(columns=[\"code\"])\n",
    "# )\n",
    "\n",
    "# Load provider info for tables that need it\n",
    "prov = pd.read_csv(Path(DATA_DIR) / \"provider.csv.csv\", low_memory=False)\n",
    "\n",
    "# Merging Prescriptions, POE, and EMAR\n",
    "pres = pd.read_csv(Path(DATA_DIR) / \"prescriptions.csv_sample1000.csv\", low_memory=False)\n",
    "poe = pd.read_csv(Path(DATA_DIR) / \"poe.csv_sample1000.csv\", parse_dates=[\"ordertime\"], low_memory=False)\n",
    "emar = pd.read_csv(Path(DATA_DIR) / \"emar.csv_sample1000.csv\", parse_dates=[\"charttime\",\"storetime\"], low_memory=False)\n",
    "\n",
    "tmp = pd.merge(pres, poe, on=['poe_id','hadm_id'], how='left')\n",
    "link_tables[\"prescriptions\"] = pd.merge(tmp, emar, on=['poe_id','hadm_id'], how='left')\n",
    "\n",
    "\n",
    "# Load remaining tables with provider merging where applicable\n",
    "for tbl in [\"transfers\"]:\n",
    "    df = pd.read_csv(Path(DATA_DIR) / f\"{tbl}.csv_sample1000.csv\", low_memory=False)\n",
    "\n",
    "# Didn't see the need for provider and services\n",
    "\n",
    "    link_tables[tbl] = df\n",
    "\n",
    "# Group by hadm_id for constant‐time lookup\n",
    "grouped = {name: df.groupby(\"hadm_id\") for name, df in link_tables.items() if \"hadm_id\" in df.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "link_tables.keys()  # to see what tables we have loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "link_tables[\"labevents\"].head()  # to see the admissions table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_dir = Path(\"mimic_sample_1000/exports\")\n",
    "export_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Export admissions_df\n",
    "with open(export_dir / \"admissions_df.pkl\", \"wb\") as f:\n",
    "    pickle.dump(admissions_df, f)\n",
    "\n",
    "# Export link_tables\n",
    "with open(export_dir / \"link_tables.pkl\", \"wb\") as f:\n",
    "    pickle.dump(link_tables, f)\n",
    "\n",
    "# Export grouped tables (for convenience)\n",
    "with open(export_dir / \"grouped_tables.pkl\", \"wb\") as f:\n",
    "    pickle.dump(grouped, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "link_tables[\"microbiologyevents\"].columns  # to see the structure of the prescriptions table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "def make_section_docs(adm_row, grouped):\n",
    "    hadm = adm_row.hadm_id\n",
    "    subj = adm_row.subject_id\n",
    "    base_meta = {\n",
    "        \"hadm_id\": hadm,\n",
    "        \"subject_id\": subj,\n",
    "        \"admittime\": adm_row.admittime.isoformat() if pd.notna(adm_row.admittime) else \"N/A\",\n",
    "        \"dischtime\": adm_row.dischtime.isoformat() if pd.notna(adm_row.dischtime) else \"N/A\",\n",
    "        \"admission_type\": adm_row.admission_type\n",
    "    }\n",
    "    docs = []\n",
    "    def safe(val, default=\"N/A\"):\n",
    "        if pd.isna(val) or (isinstance(val, str) and not val.strip()):\n",
    "            return default\n",
    "        return val\n",
    "\n",
    "    # — Header\n",
    "    header = (\n",
    "        f\"Admission {hadm} (Subject {subj})\\n\"\n",
    "        f\"- Admitted: {adm_row.admittime}    Discharged: {adm_row.dischtime}\\n\"\n",
    "        f\"- Type: {adm_row.admission_type}    ExpireFlag: {adm_row.hospital_expire_flag}\"\n",
    "    )\n",
    "    docs.append(Document(page_content=header, metadata={**base_meta, \"section\":\"header\"}))\n",
    "\n",
    "    # — Diagnoses\n",
    "    if hadm in grouped[\"diagnoses_icd\"].groups:\n",
    "        df_dx = grouped[\"diagnoses_icd\"].get_group(hadm)\n",
    "        lines = [f\"{safe(row.icd_code)}: {safe(row.long_title)}\" for _, row in df_dx.iterrows()]\n",
    "        docs.append(Document(\n",
    "            page_content=\"Diagnoses (ICD):\\n\" + \"\\n\".join(lines),\n",
    "            metadata={**base_meta, \"section\":\"diagnoses\"}\n",
    "        ))\n",
    "\n",
    "    # — Procedures\n",
    "    if hadm in grouped[\"procedures_icd\"].groups:\n",
    "        df_proc = grouped[\"procedures_icd\"].get_group(hadm)\n",
    "        lines = [f\"{safe(row.icd_code)}: {safe(row.long_title)}\" for _, row in df_proc.iterrows()]\n",
    "        docs.append(Document(\n",
    "            page_content=\"Procedures (ICD):\\n\" + \"\\n\".join(lines),\n",
    "            metadata={**base_meta, \"section\":\"procedures\"}\n",
    "        ))\n",
    "    # — Labs\n",
    "    if hadm in grouped[\"labevents\"].groups:\n",
    "        df_labs = grouped[\"labevents\"].get_group(hadm)\n",
    "        lines = []\n",
    "        for _, row in df_labs.iterrows():\n",
    "            chart_time = row.charttime.strftime(\"%Y-%m-%d %H:%M\") if pd.notna(row.charttime) else \"N/A\"\n",
    "            store_time = row.storetime.strftime(\"%Y-%m-%d %H:%M\") if pd.notna(row.storetime) else \"N/A\"\n",
    "            line = f\"{safe(row.itemid)}: {safe(row.label)} - (chart time: {chart_time} ~ store time: {store_time}) {safe(row.value)} {safe(row.valuenum)} | {safe(row.label)} - {safe(row.category)} - {safe(row.fluid)} - {safe(row.priority)} | {safe(row.flag)}\"\n",
    "            lines.append(line)\n",
    "        docs.append(Document(\n",
    "            page_content=\"Labs:\\n\" + \"\\n\".join(lines),\n",
    "            metadata={**base_meta, \"section\":\"labs\"}\n",
    "        ))\n",
    "\n",
    "    # — Microbiology\n",
    "    if hadm in grouped[\"microbiologyevents\"].groups:\n",
    "        df_micro = grouped[\"microbiologyevents\"].get_group(hadm)\n",
    "        lines = []\n",
    "        for _, row in df_micro.iterrows():\n",
    "            chart_time = row.charttime.strftime(\"%Y-%m-%d %H:%M\") if pd.notna(row.charttime) else \"N/A\"\n",
    "            store_time = row.storetime.strftime(\"%Y-%m-%d %H:%M\") if pd.notna(row.storetime) else \"N/A\"\n",
    "            chart_date = row.chartdate.strftime(\"%Y-%m-%d\") if pd.notna(row.chartdate) else \"N/A\"\n",
    "            store_date = row.storedate.strftime(\"%Y-%m-%d\") if pd.notna(row.storedate) else \"N/A\"\n",
    "            line = f\"{safe(row.test_itemid)}: {safe(row.test_name)} - {safe(row.spec_type_desc)} (chart time: {chart_time} ~ store time: {store_time} ~ chart date: {chart_date} ~ store date: {store_date}) | {safe(row.comments)}\"\n",
    "            lines.append(line)\n",
    "        docs.append(Document(\n",
    "            page_content=\"Microbiology:\\n\" + \"\\n\".join(lines),\n",
    "            metadata={**base_meta, \"section\":\"microbiology\"}\n",
    "        ))\n",
    "    # — Prescriptions and EMAR AND POE\n",
    "    if hadm in grouped[\"prescriptions\"].groups:\n",
    "        df_combined = grouped[\"prescriptions\"].get_group(hadm)\n",
    "        lines = []\n",
    "        for _, row in df_combined.iterrows():\n",
    "            order_time = row.ordertime.strftime(\"%Y-%m-%d %H:%M\") if pd.notna(row.ordertime) else \"N/A\"\n",
    "            chart_time = row.charttime.strftime(\"%Y-%m-%d %H:%M\") if pd.notna(row.charttime) else \"N/A\"\n",
    "            line = (\n",
    "                f\"{safe(row.drug_type)} ({safe(row.drug)}) - {safe(row.formulary_drug_cd)} \"\n",
    "                f\"{safe(row.dose_unit_rx)} {safe(row.dose_val_rx)} {safe(row.prod_strength)} | \"\n",
    "                f\"{safe(row.doses_per_24_hrs)} doses/24hrs | Order at {safe(order_time)} ({safe(row.order_type)}, {safe(row.order_status)}) | \"\n",
    "                f\"Administered: {safe(row.medication)} at {safe(chart_time)}\"\n",
    "            )\n",
    "            lines.append(line)\n",
    "        page_content = \"Combined Prescriptions, Orders, and Administration:\\n\" + \"\\n\".join(lines)\n",
    "        docs.append(Document(\n",
    "            page_content=page_content,\n",
    "            metadata={**base_meta, \"section\": \"prescriptions\"}\n",
    "        ))\n",
    "    return docs\n",
    "\n",
    "# build a flat list of section‐level docs\n",
    "section_docs = []\n",
    "for _, adm in admissions_df.iterrows():\n",
    "    section_docs.extend(make_section_docs(adm, grouped))\n",
    "\n",
    "print(f\"Emitted {len(section_docs)} small Documents.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "chunked_docs = splitter.split_documents(section_docs)\n",
    "print(f\"→ {len(chunked_docs)} total chunks ready for embedding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in chunked_docs:\n",
    "    # keep only what you filter on downstream:\n",
    "    md = {\n",
    "      \"hadm_id\": d.metadata[\"hadm_id\"],\n",
    "      \"subject_id\": d.metadata[\"subject_id\"],\n",
    "      \"section\": d.metadata[\"section\"],\n",
    "      \"admittime\": pd.to_datetime(d.metadata[\"admittime\"]),\n",
    "      \"dischtime\": pd.to_datetime(d.metadata[\"dischtime\"]),\n",
    "    }\n",
    "    d.metadata = md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "clinical_emb = SentenceTransformerEmbeddings(\n",
    "    model_name=\"../models/S-PubMedBert-MS-MARCO\",\n",
    "    encode_kwargs={\"batch_size\": 16}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [doc.page_content for doc in chunked_docs[:5]]\n",
    "vectors = clinical_emb.embed_documents(texts)\n",
    "print([len(v) for v in vectors])  # should each be e.g. 768-dimensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Save the chunked documents to a file\n",
    "with open(\"../mimic_sample_1000/chunked_docs.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunked_docs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "Loading different embedding models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Model: all-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "local_model_dir = Path(\"../models/all-MiniLM-L6-v2\")\n",
    "local_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# First, download and save the model\n",
    "model = SentenceTransformer(model_name)\n",
    "model.save(str(local_model_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "clinical_emb = SentenceTransformerEmbeddings(\n",
    "    model_name=\"../models/all-MiniLM-L6-v2\",\n",
    "    encode_kwargs={\"batch_size\": 16}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "vectorstore = FAISS.from_documents(chunked_docs, clinical_emb)\n",
    "vectorstore.save_local(\"../vector_stores/faiss_mimic_sample1000_mini-lm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Model: S-PubMedBert-MS-MARCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "local_model_dir = Path(\"../models/S-PubMedBert-MS-MARCO\")\n",
    "local_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_name = \"pritamdeka/S-PubMedBert-MS-MARCO\"\n",
    "\n",
    "# First, download and save the model\n",
    "model = SentenceTransformer(model_name)\n",
    "model.save(str(local_model_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "clinical_emb = SentenceTransformerEmbeddings(\n",
    "    model_name=\"../models/S-PubMedBert-MS-MARCO\",\n",
    "    encode_kwargs={\"batch_size\": 16}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "vectorstore = FAISS.from_documents(chunked_docs, clinical_emb)\n",
    "vectorstore.save_local(\"../vector_stores/faiss_mimic_sample1000_ms-marco\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "Model: static-retrieval-mrl-en-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "local_model_dir = Path(\"../models/static-retrieval-mrl-en-v1\")\n",
    "local_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_name = \"sentence-transformers/static-retrieval-mrl-en-v1\"\n",
    "\n",
    "# First, download and save the model\n",
    "model = SentenceTransformer(model_name)\n",
    "model.save(str(local_model_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "clinical_emb = SentenceTransformerEmbeddings(\n",
    "    model_name=\"../models/static-retrieval-mrl-en-v1\",\n",
    "    encode_kwargs={\"batch_size\": 16}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "vectorstore = FAISS.from_documents(chunked_docs, clinical_emb)\n",
    "vectorstore.save_local(\"../vector_stores/faiss_mimic_sample1000_static-retr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "Model: multi-qa-mpnet-base-cos-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "local_model_dir = Path(\"../models/multi-qa-mpnet-base-cos-v1\")\n",
    "local_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_name = \"sentence-transformers/multi-qa-mpnet-base-cos-v1\"\n",
    "\n",
    "# First, download and save the model\n",
    "model = SentenceTransformer(model_name)\n",
    "model.save(str(local_model_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "clinical_emb = SentenceTransformerEmbeddings(\n",
    "    model_name=\"./models/multi-qa-mpnet-base-cos-v1\",\n",
    "    encode_kwargs={\"batch_size\": 16}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "with open(\"../mimic_sample_1000/chunked_docs.pkl\", \"rb\") as f:\n",
    "    chunked_docs = pickle.load(f)\n",
    "\n",
    "\n",
    "vectorstore = FAISS.from_documents(chunked_docs, clinical_emb)\n",
    "\n",
    "\n",
    "vectorstore.save_local(\"../vector_stores/faiss_mimic_sample1000_multi-qa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "Model: BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "import pickle\n",
    "with open(\"../mimic_sample_1000/chunked_docs.pkl\", \"rb\") as f:\n",
    "    chunked_docs = pickle.load(f)\n",
    "\n",
    "# Use HuggingFaceEmbeddings for BiomedBERT\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "clinical_emb = HuggingFaceEmbeddings(\n",
    "    model_name=\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\",\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(chunked_docs, clinical_emb)\n",
    "\n",
    "\n",
    "vectorstore.save_local(\"../vector_stores/faiss_mimic_sample1000_biomedbert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "Model: all-mpnet-base-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "local_model_dir = Path(\n",
    "    \"../models/all-mpnet-base-v2\")\n",
    "local_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "# First, download and save the model\n",
    "model = SentenceTransformer(model_name)\n",
    "model.save(str(local_model_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "clinical_emb = SentenceTransformerEmbeddings(\n",
    "    model_name=\"../models/all-mpnet-base-v2\",\n",
    "    encode_kwargs={\"batch_size\": 16}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "with open(\"../mimic_sample_1000/chunked_docs.pkl\", \"rb\") as f:\n",
    "    chunked_docs = pickle.load(f)\n",
    "\n",
    "\n",
    "vectorstore = FAISS.from_documents(chunked_docs, clinical_emb)\n",
    "\n",
    "\n",
    "vectorstore.save_local(\"../vector_stores/faiss_mimic_sample1000_mpnet-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "Model: e5-base-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "local_model_dir = Path(\n",
    "    \"../models/e5-base-v2\")\n",
    "local_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_name = \"intfloat/e5-base-v2\"\n",
    "\n",
    "# First, download and save the model\n",
    "model = SentenceTransformer(model_name)\n",
    "model.save(str(local_model_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "clinical_emb = SentenceTransformerEmbeddings(\n",
    "    model_name=\"../models/e5-base-v2\",\n",
    "    encode_kwargs={\"batch_size\": 16}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "with open(\"../mimic_sample_1000/chunked_docs.pkl\", \"rb\") as f:\n",
    "    chunked_docs = pickle.load(f)\n",
    "\n",
    "\n",
    "vectorstore = FAISS.from_documents(chunked_docs, clinical_emb)\n",
    "\n",
    "\n",
    "vectorstore.save_local(\"../vector_stores/faiss_mimic_sample1000_e5-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "TimKond/S-PubMedBert-MedQuAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "local_model_dir = Path(\n",
    "    \"../models/S-PubMedBert-MedQuAD\")\n",
    "local_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_name = \"TimKond/S-PubMedBert-MedQuAD\"\n",
    "\n",
    "# First, download and save the model\n",
    "model = SentenceTransformer(model_name)\n",
    "model.save(str(local_model_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "clinical_emb = SentenceTransformerEmbeddings(\n",
    "    model_name=\"../models/S-PubMedBert-MedQuAD\",\n",
    "    encode_kwargs={\"batch_size\": 16}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "with open(\"../mimic_sample_1000/chunked_docs.pkl\", \"rb\") as f:\n",
    "    chunked_docs = pickle.load(f)\n",
    "\n",
    "\n",
    "vectorstore = FAISS.from_documents(chunked_docs, clinical_emb)\n",
    "\n",
    "\n",
    "vectorstore.save_local(\n",
    "    \"../vector_stores/faiss_mimic_sample1000_MedQuAD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "local_model_dir = Path(\n",
    "    \"../models/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\")\n",
    "local_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_name = \"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\"\n",
    "\n",
    "# First, download and save the model\n",
    "model = SentenceTransformer(model_name)\n",
    "model.save(str(local_model_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "clinical_emb = SentenceTransformerEmbeddings(\n",
    "    model_name=\"../models/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\",\n",
    "    encode_kwargs={\"batch_size\": 16}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "with open(\"../mimic_sample_1000/chunked_docs.pkl\", \"rb\") as f:\n",
    "    chunked_docs = pickle.load(f)\n",
    "\n",
    "\n",
    "vectorstore = FAISS.from_documents(chunked_docs, clinical_emb)\n",
    "\n",
    "\n",
    "vectorstore.save_local(\"../vector_stores/faiss_mimic_sample1000_BioBERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "FremyCompany/BioLORD-2023-C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "local_model_dir = Path(\n",
    "    \"../models/BioLORD-2023-C\")\n",
    "local_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_name = \"FremyCompany/BioLORD-2023-C\"\n",
    "\n",
    "# First, download and save the model\n",
    "model = SentenceTransformer(model_name)\n",
    "model.save(str(local_model_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "clinical_emb = SentenceTransformerEmbeddings(\n",
    "    model_name=\"../models/BioLORD-2023-C\",\n",
    "    encode_kwargs={\"batch_size\": 16}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "with open(\"../mimic_sample_1000/chunked_docs.pkl\", \"rb\") as f:\n",
    "    chunked_docs = pickle.load(f)\n",
    "\n",
    "\n",
    "vectorstore = FAISS.from_documents(chunked_docs, clinical_emb)\n",
    "\n",
    "\n",
    "vectorstore.save_local(\"../vector_stores/faiss_mimic_sample1000_BioLORD\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

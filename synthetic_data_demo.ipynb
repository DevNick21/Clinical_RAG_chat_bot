{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2e3db70",
   "metadata": {},
   "source": [
    "# Clinical RAG System with Synthetic Data\n",
    "\n",
    "This notebook demonstrates how to use the synthetic data generation system to work with the Clinical RAG system without requiring access to the real MIMIC-IV dataset. The synthetic data mimics the structure and characteristics of the MIMIC-IV data, allowing for development, testing, and demonstration of the RAG system.\n",
    "\n",
    "## What You'll Learn\n",
    "- How to generate synthetic medical data\n",
    "- How to process the data into documents for the RAG system\n",
    "- How to create vector stores from the synthetic data\n",
    "- How to query the RAG system with clinical questions\n",
    "- How to customize the synthetic data generation\n",
    "\n",
    "This notebook is particularly useful for:\n",
    "- Users without MIMIC-IV access\n",
    "- Development and testing\n",
    "- Educational purposes\n",
    "- Public demonstrations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27132cb7",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "Let's start by importing the necessary libraries and setting up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f974361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Make sure the project directory is in the path (for imports)\n",
    "project_root = Path(os.getcwd())\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "# Import RAG system components\n",
    "from RAG_chat_pipeline.config import model_in_use, llms, LLM_MODEL\n",
    "from RAG_chat_pipeline.utils.data_provider import DataProvider\n",
    "from RAG_chat_pipeline.utils.synthetic_data.synthetic_data_generator import SyntheticDataGenerator\n",
    "from RAG_chat_pipeline.embeddings_manager import initialize_embeddings, load_or_create_vector_stores\n",
    "from RAG_chat_pipeline.clinical_rag import ClinicalRAG\n",
    "\n",
    "# Check if the necessary components are available\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Default embedding model: {model_in_use}\")\n",
    "print(f\"Default LLM model: {LLM_MODEL['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0f4d0a",
   "metadata": {},
   "source": [
    "## 2. Generate Synthetic Medical Data\n",
    "\n",
    "Now we'll generate synthetic medical data using the `SyntheticDataGenerator` class. This data will mimic the structure of the MIMIC-IV dataset, including patient admissions, diagnoses, procedures, lab values, and medications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d03e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synthetic data generator instance\n",
    "synthetic_gen = SyntheticDataGenerator()\n",
    "\n",
    "# Set parameters for synthetic data generation\n",
    "num_patients = 100  # Number of synthetic patients to generate\n",
    "num_admissions = 150  # Total number of admissions (some patients have multiple)\n",
    "\n",
    "print(f\"Generating synthetic data for {num_patients} patients with {num_admissions} total admissions...\")\n",
    "\n",
    "# Generate the synthetic data\n",
    "# This will create CSV files in the mimic_sample_1000 directory with synthetic data\n",
    "synthetic_gen.generate_data(num_patients=num_patients, num_admissions=num_admissions)\n",
    "\n",
    "# List the generated files\n",
    "mimic_dir = Path(project_root) / \"mimic_sample_1000\"\n",
    "synthetic_files = list(mimic_dir.glob(\"*_sample*.csv\"))\n",
    "print(f\"Generated {len(synthetic_files)} synthetic data files:\")\n",
    "for file in synthetic_files:\n",
    "    print(f\"  - {file.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892c3e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the synthetic admissions data\n",
    "admissions_path = mimic_dir / \"admissions.csv_sample1000.csv\"\n",
    "if admissions_path.exists():\n",
    "    admissions_df = pd.read_csv(admissions_path)\n",
    "    print(f\"\\nSynthetic Admissions Data Sample (First 5 rows):\")\n",
    "    display(admissions_df.head())\n",
    "    \n",
    "    print(f\"\\nTotal admissions: {len(admissions_df)}\")\n",
    "    print(f\"Unique patients: {admissions_df['subject_id'].nunique()}\")\n",
    "    print(f\"Admission types: {admissions_df['admission_type'].unique()}\")\n",
    "else:\n",
    "    print(\"Admissions data file not found. Make sure synthetic data was generated correctly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2ff3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also look at diagnoses and lab values\n",
    "diagnoses_path = mimic_dir / \"diagnoses_icd.csv_sample1000.csv\"\n",
    "labs_path = mimic_dir / \"labevents.csv_sample1000.csv\"\n",
    "\n",
    "if diagnoses_path.exists():\n",
    "    diagnoses_df = pd.read_csv(diagnoses_path)\n",
    "    print(f\"\\nSynthetic Diagnoses Data Sample (First 5 rows):\")\n",
    "    display(diagnoses_df.head())\n",
    "    print(f\"Total diagnoses: {len(diagnoses_df)}\")\n",
    "    print(f\"Unique ICD codes: {diagnoses_df['icd_code'].nunique()}\")\n",
    "else:\n",
    "    print(\"Diagnoses data file not found.\")\n",
    "\n",
    "if labs_path.exists():\n",
    "    labs_df = pd.read_csv(labs_path)\n",
    "    print(f\"\\nSynthetic Lab Events Data Sample (First 5 rows):\")\n",
    "    display(labs_df.head())\n",
    "    print(f\"Total lab events: {len(labs_df)}\")\n",
    "    print(f\"Unique lab items: {labs_df['itemid'].nunique()}\")\n",
    "    \n",
    "    # Show distribution of abnormal flags\n",
    "    if 'flag' in labs_df.columns:\n",
    "        flag_counts = labs_df['flag'].value_counts()\n",
    "        print(\"\\nDistribution of abnormal flags:\")\n",
    "        display(flag_counts)\n",
    "else:\n",
    "    print(\"Lab events data file not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc151473",
   "metadata": {},
   "source": [
    "## 3. Process Data into Documents\n",
    "\n",
    "Now that we have synthetic data, we need to process it into documents suitable for the RAG pipeline. This involves:\n",
    "1. Loading the data\n",
    "2. Organizing it into patient-specific documents\n",
    "3. Creating semantic chunks optimized for retrieval\n",
    "\n",
    "The `DataProvider` class handles the abstraction between real and synthetic data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422d4992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the data provider to work with our data\n",
    "data_provider = DataProvider(project_root=project_root)\n",
    "\n",
    "# Check if we're working with synthetic or real data\n",
    "print(f\"Using synthetic data: {data_provider.using_synthetic_data}\")\n",
    "\n",
    "# Process the data into documents if not already done\n",
    "chunked_docs_path = mimic_dir / \"chunked_docs.pkl\"\n",
    "\n",
    "if chunked_docs_path.exists():\n",
    "    print(\"Loading existing document chunks...\")\n",
    "    with open(chunked_docs_path, 'rb') as f:\n",
    "        chunked_docs = pickle.load(f)\n",
    "    print(f\"Loaded {len(chunked_docs)} document chunks\")\n",
    "else:\n",
    "    print(\"Processing data into document chunks...\")\n",
    "    # This would normally be a longer process involving:\n",
    "    # 1. Loading admission data\n",
    "    # 2. Joining with diagnoses, procedures, lab values, etc.\n",
    "    # 3. Creating text documents for each admission\n",
    "    # 4. Chunking the documents for better retrieval\n",
    "    \n",
    "    # For this demo, we'll force the data provider to generate chunks\n",
    "    chunked_docs = data_provider.get_chunked_docs()\n",
    "    print(f\"Generated {len(chunked_docs)} document chunks\")\n",
    "    \n",
    "    # Save the chunks for future use\n",
    "    with open(chunked_docs_path, 'wb') as f:\n",
    "        pickle.dump(chunked_docs, f)\n",
    "    print(f\"Saved document chunks to {chunked_docs_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780467c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine a few document chunks\n",
    "if chunked_docs:\n",
    "    print(\"\\nExample Document Chunks:\")\n",
    "    for i, doc in enumerate(chunked_docs[:3]):\n",
    "        print(f\"\\n--- Document Chunk {i+1} ---\")\n",
    "        print(f\"Metadata: {doc.metadata}\")\n",
    "        print(f\"Content Sample: {doc.page_content[:150]}...\")\n",
    "        print(\"-------------------\")\n",
    "        \n",
    "    # Count chunks by type\n",
    "    chunk_types = {}\n",
    "    for doc in chunked_docs:\n",
    "        section_type = doc.metadata.get('section_type', 'unknown')\n",
    "        chunk_types[section_type] = chunk_types.get(section_type, 0) + 1\n",
    "    \n",
    "    print(\"\\nDocument Chunks by Type:\")\n",
    "    for chunk_type, count in chunk_types.items():\n",
    "        print(f\"  {chunk_type}: {count} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6759d6d8",
   "metadata": {},
   "source": [
    "## 4. Create Vector Store with Embeddings\n",
    "\n",
    "Now we'll create a vector store from our document chunks using embeddings. This will allow for semantic search of the clinical text. We'll use the default embedding model specified in the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ade9ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the embedding model (using the default from config.py)\n",
    "embeddings = initialize_embeddings(model_name=model_in_use)\n",
    "\n",
    "print(f\"Using embedding model: {model_in_use}\")\n",
    "\n",
    "# Create or load the vector store\n",
    "vector_store_path = Path(project_root) / \"vector_stores\" / f\"faiss_mimic_sample1000_{model_in_use}\"\n",
    "print(f\"Vector store path: {vector_store_path}\")\n",
    "\n",
    "if vector_store_path.exists():\n",
    "    print(\"Vector store already exists. Loading existing vector store...\")\n",
    "    # In the actual system, load_or_create_vector_stores would handle this\n",
    "else:\n",
    "    print(\"Creating new vector store from document chunks...\")\n",
    "    \n",
    "# Get the vector store (this will create it if it doesn't exist)\n",
    "vector_store = load_or_create_vector_stores(\n",
    "    docs=chunked_docs,\n",
    "    model_name=model_in_use,\n",
    "    embeddings=embeddings\n",
    ")\n",
    "\n",
    "print(f\"Vector store ready for querying: {type(vector_store).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635f6e1b",
   "metadata": {},
   "source": [
    "## 5. Initialize RAG Pipeline\n",
    "\n",
    "With our vector store ready, we can now initialize the RAG pipeline. This will connect our synthetic data with a language model to enable question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76783146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RAG pipeline\n",
    "print(\"Initializing RAG pipeline...\")\n",
    "\n",
    "try:\n",
    "    # Create the ClinicalRAG instance\n",
    "    rag = ClinicalRAG(\n",
    "        vector_store=vector_store,\n",
    "        embeddings=embeddings,\n",
    "        llm_model=LLM_MODEL,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    print(f\"RAG pipeline initialized successfully.\")\n",
    "    print(f\"Using LLM model: {LLM_MODEL['name']}\")\n",
    "    print(f\"Using embeddings model: {model_in_use}\")\n",
    "    \n",
    "    # Initialize conversation history\n",
    "    conversation_history = []\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error initializing RAG pipeline: {str(e)}\")\n",
    "    print(\"This could be due to Ollama not running or the LLM model not being available.\")\n",
    "    print(\"Please ensure Ollama is installed and the specified model is pulled.\")\n",
    "    print(f\"You may need to run: ollama pull {LLM_MODEL['name']}\")\n",
    "    \n",
    "    # For demo purposes, we'll create a mock RAG object if there's an error\n",
    "    print(\"\\nCreating mock RAG object for demonstration...\")\n",
    "    class MockRAG:\n",
    "        def ask_question(self, query, k=5, admission_id=None, conversation_history=None):\n",
    "            return {\n",
    "                \"answer\": f\"[MOCK] This is a simulated response to: '{query}'\",\n",
    "                \"source_documents\": [],\n",
    "                \"search_time\": 0.1,\n",
    "                \"total_time\": 0.2\n",
    "            }\n",
    "    rag = MockRAG()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d619ae50",
   "metadata": {},
   "source": [
    "## 6. Query the System with Clinical Questions\n",
    "\n",
    "Now we're ready to query our RAG system using clinical questions. We'll demonstrate different types of queries that would be typical in a clinical setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2a8e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to ask questions and display results\n",
    "def ask_clinical_question(question, admission_id=None, k=5):\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Admission ID: {admission_id if admission_id else 'None (global search)'}\")\n",
    "    print(f\"Retrieved documents: {k}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    response = rag.ask_question(\n",
    "        query=question,\n",
    "        k=k,\n",
    "        admission_id=admission_id,\n",
    "        conversation_history=conversation_history\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Add to conversation history\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": question})\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": response[\"answer\"]})\n",
    "    \n",
    "    # Display the results\n",
    "    print(\"Answer:\")\n",
    "    print(response[\"answer\"])\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Total response time: {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"Search time: {response.get('search_time', 0):.2f} seconds\")\n",
    "    \n",
    "    # Display source documents\n",
    "    if \"source_documents\" in response and response[\"source_documents\"]:\n",
    "        print(\"\\nSource Documents:\")\n",
    "        for i, doc in enumerate(response[\"source_documents\"]):\n",
    "            print(f\"\\nDocument {i+1}:\")\n",
    "            print(f\"  Score: {doc.metadata.get('score', 'N/A')}\")\n",
    "            print(f\"  Metadata: {doc.metadata}\")\n",
    "            # Print truncated content\n",
    "            content = doc.page_content\n",
    "            if len(content) > 100:\n",
    "                content = content[:100] + \"...\"\n",
    "            print(f\"  Content: {content}\")\n",
    "    else:\n",
    "        print(\"\\nNo source documents returned.\")\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70abdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a random admission ID from our synthetic data for testing\n",
    "try:\n",
    "    admissions_df = pd.read_csv(mimic_dir / \"admissions.csv_sample1000.csv\")\n",
    "    sample_admission_id = str(admissions_df['hadm_id'].sample(1).values[0])\n",
    "    print(f\"Selected random admission ID for testing: {sample_admission_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading sample admission ID: {str(e)}\")\n",
    "    sample_admission_id = \"12345678\"  # Fallback ID for testing\n",
    "\n",
    "# Example 1: General question about an admission\n",
    "print(\"\\n\\n--- Example 1: General Admission Information ---\")\n",
    "question1 = f\"What was the reason for admission {sample_admission_id}?\"\n",
    "response1 = ask_clinical_question(question1, admission_id=sample_admission_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da9b91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Specific diagnostic question\n",
    "print(\"\\n\\n--- Example 2: Diagnostic Information ---\")\n",
    "question2 = f\"What diagnoses were made for admission {sample_admission_id}?\"\n",
    "response2 = ask_clinical_question(question2, admission_id=sample_admission_id)\n",
    "\n",
    "# Example 3: Lab values question\n",
    "print(\"\\n\\n--- Example 3: Laboratory Values ---\")\n",
    "question3 = f\"Were there any abnormal lab values for admission {sample_admission_id}?\"\n",
    "response3 = ask_clinical_question(question3, admission_id=sample_admission_id)\n",
    "\n",
    "# Example 4: Medication question\n",
    "print(\"\\n\\n--- Example 4: Medications ---\")\n",
    "question4 = f\"What medications were prescribed for admission {sample_admission_id}?\"\n",
    "response4 = ask_clinical_question(question4, admission_id=sample_admission_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1a4b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5: Follow-up question (using conversation history)\n",
    "print(\"\\n\\n--- Example 5: Follow-up Question ---\")\n",
    "question5 = \"Were any of these medications for pain management?\"\n",
    "response5 = ask_clinical_question(question5, admission_id=sample_admission_id)\n",
    "\n",
    "# Example 6: Global question (across all patients)\n",
    "print(\"\\n\\n--- Example 6: Global Question ---\")\n",
    "question6 = \"How many patients had hypertension as a diagnosis?\"\n",
    "response6 = ask_clinical_question(question6, admission_id=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd07cf94",
   "metadata": {},
   "source": [
    "## 7. Customize Synthetic Data Generation\n",
    "\n",
    "The synthetic data generation system can be customized to create specific medical scenarios or to increase the variety of conditions. Here we'll demonstrate how to customize the data generation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f6ceca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom synthetic data generator\n",
    "custom_synthetic_gen = SyntheticDataGenerator()\n",
    "\n",
    "# Customize the generator by adding specific conditions\n",
    "# For example, to increase the likelihood of respiratory conditions:\n",
    "custom_synthetic_gen.common_diagnoses.extend([\n",
    "    {\"icd_code\": \"J44.9\", \"description\": \"Chronic obstructive pulmonary disease, unspecified\"},\n",
    "    {\"icd_code\": \"J45.909\", \"description\": \"Unspecified asthma, uncomplicated\"},\n",
    "    {\"icd_code\": \"J18.9\", \"description\": \"Pneumonia, unspecified organism\"}\n",
    "])\n",
    "\n",
    "# Customize lab value generation to include more abnormal results\n",
    "custom_synthetic_gen.abnormal_lab_probability = 0.4  # Default is typically lower\n",
    "\n",
    "# Customize admission types to include more emergency admissions\n",
    "custom_synthetic_gen.admission_type_weights = {\n",
    "    'EMERGENCY': 0.6,  # Increased probability\n",
    "    'ELECTIVE': 0.3,\n",
    "    'URGENT': 0.1\n",
    "}\n",
    "\n",
    "print(\"Customized synthetic data generator settings:\")\n",
    "print(f\"- Added specific respiratory conditions\")\n",
    "print(f\"- Increased abnormal lab probability to 40%\")\n",
    "print(f\"- Modified admission type distribution: 60% emergency, 30% elective, 10% urgent\")\n",
    "\n",
    "# To generate data with these custom settings:\n",
    "# custom_synthetic_gen.generate_data(num_patients=100, num_admissions=150, output_dir=\"custom_synthetic_data\")\n",
    "\n",
    "# Note: Uncommenting the line above would generate new synthetic data with the custom settings\n",
    "# For this demo, we'll just show the customization options without actually generating new data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94614418",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to use the synthetic data generation system with the Clinical RAG pipeline. The synthetic data option provides several advantages:\n",
    "\n",
    "1. **No MIMIC-IV Credentials Required**: You can develop and test without needing access to the real dataset\n",
    "2. **Customizable Scenarios**: You can generate specific medical scenarios for testing or educational purposes\n",
    "3. **Shareable Code**: The synthetic data can be committed to Git, making it easy to share your work\n",
    "4. **Seamless Integration**: The system automatically detects and uses synthetic data if real data isn't available\n",
    "\n",
    "The RAG system works identically with both synthetic and real data, making it easy to transition between them as needed.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try customizing the synthetic data generation for specific scenarios\n",
    "- Experiment with different embedding models and LLMs\n",
    "- Use the web interface to interact with the system\n",
    "- Contribute to improving the synthetic data generation for more realistic scenarios"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
